# ğŸ“˜ Module 5 â€“ Chapitre 4 : Visualisation des rÃ©sultats de modÃ¨les

## ğŸ¯ Objectif
Au-delÃ  des mÃ©triques classiques (prÃ©cision, rappel, F1-score), la **visualisation** permet de comprendre *comment* un modÃ¨le prend ses dÃ©cisions, oÃ¹ il rÃ©ussit et oÃ¹ il Ã©choue.

---

## ğŸ“š Ce que vous allez apprendre
- Visualiser les **frontiÃ¨res de dÃ©cision**
- Analyser les **prÃ©dictions vs rÃ©alitÃ©**
- Comprendre lâ€™**importance des features**
- Diagnostiquer les **erreurs de modÃ¨les**
- CrÃ©er des **rapports visuels complets**

---

## â“ Pourquoi visualiser ?
Les mÃ©triques numÃ©riques **ne racontent qu'une partie de l'histoire**.

> Exemple : un modÃ¨le avec 95 % de prÃ©cision peut encore cacher des biais importants ou des erreurs systÃ©matiques.

---

## âœ… Avantages de la visualisation
- ğŸ” **Identifier** les patterns d'erreurs  
- âš–ï¸ **Comprendre** les biais du modÃ¨le  
- ğŸ§ª **Valider** les hypothÃ¨ses initiales  
- ğŸ“Š **Communiquer** les rÃ©sultats efficacement  
- ğŸ¤ **AmÃ©liorer la confiance** envers le modÃ¨le  

---

## ğŸ” Ce quâ€™on peut dÃ©couvrir
- Classes **mal sÃ©parÃ©es**
- **Overfitting** localisÃ© dans certaines zones
- Features **non pertinentes**
- DonnÃ©es **aberrantes** influentes
- RÃ©gions dâ€™**incertitude**

---

## ğŸ› ï¸ Types de visualisations utiles
1. **FrontiÃ¨res de dÃ©cision** : montrer comment un modÃ¨le sÃ©pare les classes dans un espace 2D.  
2. **Courbes PrÃ©dictions vs RÃ©alitÃ©** : scatter plots, rÃ©sidus, diagonale idÃ©ale.  
3. **Importance des features** : bar charts, SHAP values, permutation importance.  
4. **Matrice de confusion** : erreurs par classe.  
5. **Courbes ROC et PR** : comparer plusieurs modÃ¨les.  

---

## âœ… Conclusion
La visualisation complÃ¨te les mÃ©triques numÃ©riques.  
Elle rÃ©vÃ¨le :
- des **forces** : classes bien discriminÃ©es, variables pertinentes  
- des **faiblesses** : erreurs systÃ©matiques, biais, overfitting  

ğŸ‘‰ En combinant **chiffres + graphiques**, on obtient une vision plus juste et plus exploitable des performances dâ€™un modÃ¨le.

## 1. FrontiÃ¨res de dÃ©cision en 2D
Objectif : Comprendre comment le modÃ¨le sÃ©pare les classes dans l'espace des features

![FrontiÃ¨res de dÃ©cision](image/frontieres-de-decision-completes-2D.JPG)

Analyse des frontiÃ¨res de dÃ©cision:
==================================================
Logistic Regression: 0.880
Random Forest: 0.960
SVM (RBF): 0.920
SVM (Linear): 0.910

Meilleur modÃ¨le: Random Forest (0.960)

Analyse de la complexitÃ© des frontiÃ¨res:
â€¢ Logistic Regression: FrontiÃ¨re linÃ©aire simple
â€¢ SVM Linear: FrontiÃ¨re linÃ©aire optimale
â€¢ SVM RBF: FrontiÃ¨re non-linÃ©aire flexible
â€¢ Random Forest: FrontiÃ¨re non-linÃ©aire par rÃ©gions

PrÃ©dictions sur points de test:
Point		LogReg	RF	SVM-RBF	SVM-Lin
--------------------------------------------------
( 0,  0)		0	0	1	1
( 2,  2)		1	1	1	1
(-2, -2)		0	1	0	0
( 1, -1)		0	0	0	0

Analyse de confiance sur les points de test:

Point (0, 0):
  LogReg: Classe 0 (conf: 0.699)
  RF: Classe 0 (conf: 0.930)
  SVM-RBF: Classe 0 (conf: 0.512)

Point (2, 2):
  LogReg: Classe 1 (conf: 0.991)
  RF: Classe 1 (conf: 0.980)
  SVM-RBF: Classe 1 (conf: 0.936)

Point (-2, -2):
  LogReg: Classe 0 (conf: 0.998)
  RF: Classe 1 (conf: 0.560)
  SVM-RBF: Classe 0 (conf: 0.774)

Point (1, -1):
  LogReg: Classe 0 (conf: 0.934)
  RF: Classe 0 (conf: 0.990)
  SVM-RBF: Classe 0 (conf: 1.000)
  
### ğŸ” Analyse des modÃ¨les
- **Logistic Regression (Acc = 0.88)** : frontiÃ¨re linÃ©aire simple.  
- **Random Forest (Acc = 0.96)** : frontiÃ¨re complexe, adaptÃ©e aux donnÃ©es non-linÃ©aires.  
- **SVM (RBF) (Acc = 0.92)** : frontiÃ¨re non-linÃ©aire souple.  
- **SVM (Lin) (Acc = 0.91)** : frontiÃ¨re linÃ©aire, moins flexible.  

### ğŸ“ˆ Conclusion
- Le **Random Forest** est le meilleur modÃ¨le avec **96% de prÃ©cision**.  
- La complexitÃ© de la frontiÃ¨re reflÃ¨te la capacitÃ© dâ€™adaptation du modÃ¨le aux structures cachÃ©es des donnÃ©es.

## âŒ 2. Analyse des erreurs de classification â€“ Random Forest
Objectif : Identifier oÃ¹ et pourquoi le modÃ¨le se trompe

![Erreurs du modÃ¨le](image/Identifier-ou-et-pourquoi-le-modele-se-trompe.JPG)

Analyse dÃ©taillÃ©e des erreurs - Random Forest:
============================================================
Nombre total de prÃ©dictions: 100
PrÃ©dictions correctes: 96
PrÃ©dictions incorrectes: 4
Taux d'erreur: 0.040

Analyse par classe:

Classe 0:
  â€¢ Ã‰chantillons: 51
  â€¢ Correctes: 48
  â€¢ Taux de rÃ©ussite: 0.941
  â€¢ Confiance moyenne: 0.938
  â€¢ Confiance min: 0.510
  â€¢ Confiance max: 1.000

Classe 1:
  â€¢ Ã‰chantillons: 49
  â€¢ Correctes: 48
  â€¢ Taux de rÃ©ussite: 0.980
  â€¢ Confiance moyenne: 0.932
  â€¢ Confiance min: 0.590
  â€¢ Confiance max: 1.000

Analyse des prÃ©dictions incertaines:
PrÃ©dictions avec confiance < 0.6: 4
PrÃ©cision sur prÃ©dictions incertaines: 0.750
Exemples de prÃ©dictions incertaines:
  â€¢ RÃ©el: 1, PrÃ©dit: 1, Confiance: 0.590
  â€¢ RÃ©el: 1, PrÃ©dit: 1, Confiance: 0.590
  â€¢ RÃ©el: 0, PrÃ©dit: 1, Confiance: 0.510
  â€¢ RÃ©el: 0, PrÃ©dit: 0, Confiance: 0.570

Analyse spatiale des erreurs:
CentroÃ¯de des erreurs: (0.27, 0.35)
Dispersion des erreurs - Feature 1: 0.319, Feature 2: 0.173

Rapport de classification dÃ©taillÃ©:
              precision    recall  f1-score   support
    Classe 0       0.98      0.94      0.96        51
    Classe 1       0.94      0.98      0.96        49
    accuracy                           0.96       100
   macro avg       0.96      0.96      0.96       100
weighted avg       0.96      0.96      0.96       100


Recommandations pour amÃ©liorer le modÃ¨le:

### ğŸ” RÃ©sultats
- **Nombre total de prÃ©dictions** : 100  
- **PrÃ©dictions correctes** : 96  
- **PrÃ©dictions incorrectes** : 4  
- **Taux dâ€™erreur** : 4%  

### ğŸ“Š Visualisations
- **Matrice de confusion** : quelques confusions entre classe 0 et 1.  
- **Distribution des probabilitÃ©s** : modÃ¨le confiant (> 0.9) dans la majoritÃ© des cas.  
- **Localisation des erreurs** : les erreurs sont concentrÃ©es dans une zone prÃ©cise du plan.  

ğŸ‘‰ **Conclusion** : Le modÃ¨le est robuste, mais sensible dans une rÃ©gion spÃ©cifique des donnÃ©es.

# Analyse de l'importance des features
## ğŸ”‘ 1. Feature Importance et SHAP Values â€“ Gini vs Permutation Importance

Objectif : Comprendre quelles variables influencent le plus les prÃ©dictions

## Visualisation des prÃ©dictions vs rÃ©alitÃ©
# 1. Analyse pour la rÃ©gression
Objectif : Ã‰valuer la qualitÃ© des prÃ©dictions quantitatives

![Importance des Features](image/feature-importance-et-SHAP-values-quelles-variables-influencent-le-plus-predictions.JPG)


### ğŸ” Analyse
- **Top variables influentes** :  
  - *worst area, worst concave points, mean area, worst radius*.  
- **MÃ©thodes comparÃ©es** :  
  - Gini Importance (issue des arbres de dÃ©cision).  
  - Permutation Importance (mesure lâ€™impact direct sur la prÃ©diction).  

### ğŸ“ˆ Importance cumulative
- 14 features expliquent 80% de la variance.  
- 16 features â†’ 90%.  
- 20 features â†’ 95%.  

ğŸ‘‰ **Conclusion** : Peu de variables dominantes expliquent lâ€™essentiel de la performance.

## ğŸ“Š Ã‰valuer la qualitÃ© des prÃ©dictions â€“ RÃ©gression linÃ©aire vs Random Forest


![Ã‰valuer la qualitÃ© des prÃ©dictions](image/Evaluer-la-qualite-des-predictions-quantitatives.JPG)

Analyse de l'importance des features:
============================================================
Dataset: 569 Ã©chantillons, 30 features
PrÃ©cision du modÃ¨le: 0.965

Top 10 features par Gini Importance:
  0.0318 - worst concavity
  0.0476 - mean area
  0.0487 - mean radius
  0.0533 - mean perimeter
  0.0671 - worst perimeter
  0.0680 - mean concavity
  0.0780 - worst radius
  0.1062 - mean concave points
  0.1447 - worst concave points
  0.1539 - worst area

Top 10 features par Permutation Importance:
  0.0000 Â± 0.0000 - area error
  0.0000 Â± 0.0000 - worst perimeter
  0.0000 Â± 0.0000 - mean perimeter
  0.0009 Â± 0.0026 - worst concavity
  0.0009 Â± 0.0026 - mean concavity
  0.0026 Â± 0.0079 - worst area
  0.0026 Â± 0.0040 - worst texture
  0.0035 Â± 0.0043 - mean texture
  0.0035 Â± 0.0043 - mean area
  0.0035 Â± 0.0043 - worst radius

Features communes dans le top 10:
  â€¢ mean perimeter: Gini=0.0533, Perm=0.0000
  â€¢ mean area: Gini=0.0476, Perm=0.0035
  â€¢ mean concavity: Gini=0.0680, Perm=0.0009
  â€¢ worst radius: Gini=0.0780, Perm=0.0035
  â€¢ worst area: Gini=0.1539, Perm=0.0026
  â€¢ worst perimeter: Gini=0.0671, Perm=0.0000
  â€¢ worst concavity: Gini=0.0318, Perm=0.0009

Analyse de rÃ©duction dimensionnelle:
  â€¢ 11 features (63.3% de rÃ©duction) pour 80% d'importance
  â€¢ 16 features (46.7% de rÃ©duction) pour 90% d'importance
  â€¢ 20 features (33.3% de rÃ©duction) pour 95% d'importance
  â€¢ 28 features (6.7% de rÃ©duction) pour 99% d'importance

Features avec trÃ¨s faible importance (<0.001): 0

CorrÃ©lation entre Gini et Permutation Importance: -0.071
â€¢ Faible cohÃ©rence - investiguer les diffÃ©rences

Test de performance avec rÃ©duction de features:
  â€¢ Top 5 features: 0.956 (diffÃ©rence: -0.009)
  â€¢ Top 10 features: 0.956 (diffÃ©rence: -0.009)
  â€¢ Top 15 features: 0.956 (diffÃ©rence: -0.009)
  â€¢ Top 20 features: 0.965 (diffÃ©rence: +0.000)
  
### ğŸ” Analyse comparative
- **RÃ©gression LinÃ©aire** :  
  - RÂ² = 0.453 â†’ Le modÃ¨le explique 45.3% de la variance.  
  - MAE â‰ˆ 42.79 â†’ Erreur moyenne des prÃ©dictions â‰ˆ 43 unitÃ©s.  
- **Random Forest** :  
  - RÂ² = 0.443 â†’ LÃ©gÃ¨rement moins explicatif.  
  - MAE â‰ˆ 44.05 â†’ Erreur similaire.  

### ğŸ“ˆ Analyse des rÃ©sidus
- RÃ©gression linÃ©aire : les rÃ©sidus suivent une distribution normale (p-value > 0.05).  
- Random Forest : les rÃ©sidus sont un peu plus dispersÃ©s.  

ğŸ‘‰ **Conclusion** : La rÃ©gression linÃ©aire obtient un meilleur RÂ², mais les deux modÃ¨les ont une prÃ©cision proche.





