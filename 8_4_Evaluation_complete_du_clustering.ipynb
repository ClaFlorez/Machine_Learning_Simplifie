{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNypw1am8oJ9L/Hj++vqIU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaFlorez/Machine_Learning_Simplifie/blob/main/8_4_Evaluation_complete_du_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMSH036qaKbe"
      },
      "outputs": [],
      "source": [
        "#√âvaluation compl√®te du clustering\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Cr√©er plusieurs datasets avec diff√©rentes qualit√©s de clustering\n",
        "print(\"√âvaluation de la qualit√© du clustering\")\n",
        "print(\"Comparaison sur diff√©rents types de donn√©es\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dataset 1: Clusters bien s√©par√©s (facile)\n",
        "X_easy, y_easy = make_blobs(n_samples=300, centers=4, n_features=2,\n",
        "                           cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Dataset 2: Clusters proches (difficile)\n",
        "X_hard, y_hard = make_blobs(n_samples=300, centers=4, n_features=2,\n",
        "                           cluster_std=2.5, random_state=42)\n",
        "\n",
        "# Dataset 3: Clusters avec densit√©s diff√©rentes\n",
        "X_varied = []\n",
        "y_varied = []\n",
        "centers_varied = [[0, 0], [8, 8], [0, 8], [8, 0]]\n",
        "stds_varied = [0.5, 1.5, 1.0, 2.0]  # Densit√©s diff√©rentes\n",
        "\n",
        "for i, (center, std) in enumerate(zip(centers_varied, stds_varied)):\n",
        "    n_points = [100, 50, 75, 75][i]  # Tailles diff√©rentes aussi\n",
        "    cluster_points = np.random.normal(center, std, (n_points, 2))\n",
        "    X_varied.extend(cluster_points)\n",
        "    y_varied.extend([i] * n_points)\n",
        "\n",
        "X_varied = np.array(X_varied)\n",
        "y_varied = np.array(y_varied)\n",
        "\n",
        "datasets = {\n",
        "    'Facile (bien s√©par√©s)': (X_easy, y_easy),\n",
        "    'Difficile (proches)': (X_hard, y_hard),\n",
        "    'Densit√©s vari√©es': (X_varied, y_varied)\n",
        "}\n",
        "\n",
        "print(f\"Trois datasets de test cr√©√©s:\")\n",
        "for name, (X, y) in datasets.items():\n",
        "    print(f\"  {name}: {len(X)} points, {len(np.unique(y))} clusters vrais\")\n",
        "\n",
        "# Tester K-Means avec diff√©rents K sur chaque dataset\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "\n",
        "metrics_results = {}\n",
        "\n",
        "for dataset_idx, (dataset_name, (X, y_true)) in enumerate(datasets.items()):\n",
        "    print(f\"\\nAnalyse du dataset: {dataset_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Standardiser\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Tester diff√©rents nombres de clusters\n",
        "    k_range = range(2, 8)\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "\n",
        "    for k in k_range:\n",
        "        # Appliquer K-Means\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Calculer les m√©triques\n",
        "        sil_score = silhouette_score(X_scaled, clusters)\n",
        "        calinski_score = calinski_harabasz_score(X_scaled, clusters)\n",
        "        db_score = davies_bouldin_score(X_scaled, clusters)\n",
        "\n",
        "        silhouette_scores.append(sil_score)\n",
        "        calinski_scores.append(calinski_score)\n",
        "        davies_bouldin_scores.append(db_score)\n",
        "\n",
        "        print(f\"K={k}: Silhouette={sil_score:.3f}, Calinski={calinski_score:.1f}, Davies-Bouldin={db_score:.3f}\")\n",
        "\n",
        "    # Stocker les r√©sultats\n",
        "    metrics_results[dataset_name] = {\n",
        "        'silhouette': silhouette_scores,\n",
        "        'calinski': calinski_scores,\n",
        "        'davies_bouldin': davies_bouldin_scores,\n",
        "        'k_range': list(k_range)\n",
        "    }\n",
        "\n",
        "    # Visualiser les donn√©es originales\n",
        "    axes[dataset_idx, 0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='tab10', alpha=0.7)\n",
        "    axes[dataset_idx, 0].set_title(f'{dataset_name}\\n(Vraies classes)', fontweight='bold')\n",
        "\n",
        "    # Trouver le K optimal selon chaque m√©trique\n",
        "    best_k_sil = k_range[np.argmax(silhouette_scores)]\n",
        "    best_k_cal = k_range[np.argmax(calinski_scores)]\n",
        "    best_k_db = k_range[np.argmin(davies_bouldin_scores)]  # Davies-Bouldin: plus bas = mieux\n",
        "\n",
        "    print(f\"K optimal selon Silhouette: {best_k_sil}\")\n",
        "    print(f\"K optimal selon Calinski-Harabasz: {best_k_cal}\")\n",
        "    print(f\"K optimal selon Davies-Bouldin: {best_k_db}\")\n",
        "\n",
        "    # Visualiser le clustering avec K optimal (silhouette)\n",
        "    kmeans_optimal = KMeans(n_clusters=best_k_sil, random_state=42, n_init=10)\n",
        "    clusters_optimal = kmeans_optimal.fit_predict(X_scaled)\n",
        "\n",
        "    axes[dataset_idx, 1].scatter(X[:, 0], X[:, 1], c=clusters_optimal, cmap='tab10', alpha=0.7)\n",
        "    axes[dataset_idx, 1].scatter(kmeans_optimal.cluster_centers_[:, 0], kmeans_optimal.cluster_centers_[:, 1],\n",
        "                                c='red', marker='x', s=200, linewidths=3)\n",
        "    axes[dataset_idx, 1].set_title(f'K-Means (K={best_k_sil})\\nSilhouette={silhouette_scores[best_k_sil-2]:.3f}',\n",
        "                                  fontweight='bold')\n",
        "\n",
        "# Graphiques de m√©triques\n",
        "for dataset_idx, (dataset_name, metrics) in enumerate(metrics_results.items()):\n",
        "    k_range = metrics['k_range']\n",
        "\n",
        "    # Silhouette scores\n",
        "    axes[dataset_idx, 2].plot(k_range, metrics['silhouette'], 'o-', linewidth=2, markersize=8)\n",
        "    axes[dataset_idx, 2].set_title(f'Silhouette Score\\n{dataset_name}', fontweight='bold')\n",
        "    axes[dataset_idx, 2].set_xlabel('Nombre de Clusters (K)')\n",
        "    axes[dataset_idx, 2].set_ylabel('Silhouette Score')\n",
        "    axes[dataset_idx, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Marquer le maximum\n",
        "    best_idx = np.argmax(metrics['silhouette'])\n",
        "    best_k = k_range[best_idx]\n",
        "    best_score = metrics['silhouette'][best_idx]\n",
        "    axes[dataset_idx, 2].axvline(best_k, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[dataset_idx, 2].text(best_k + 0.1, best_score, f'Max: {best_score:.3f}',\n",
        "                             fontweight='bold')\n",
        "\n",
        "    # Davies-Bouldin (plus bas = mieux)\n",
        "    axes[dataset_idx, 3].plot(k_range, metrics['davies_bouldin'], 's-', linewidth=2, markersize=8, color='red')\n",
        "    axes[dataset_idx, 3].set_title(f'Davies-Bouldin Index\\n{dataset_name}', fontweight='bold')\n",
        "    axes[dataset_idx, 3].set_xlabel('Nombre de Clusters (K)')\n",
        "    axes[dataset_idx, 3].set_ylabel('Davies-Bouldin Index')\n",
        "    axes[dataset_idx, 3].grid(True, alpha=0.3)\n",
        "\n",
        "    # Marquer le minimum\n",
        "    best_idx_db = np.argmin(metrics['davies_bouldin'])\n",
        "    best_k_db = k_range[best_idx_db]\n",
        "    best_score_db = metrics['davies_bouldin'][best_idx_db]\n",
        "    axes[dataset_idx, 3].axvline(best_k_db, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[dataset_idx, 3].text(best_k_db + 0.1, best_score_db, f'Min: {best_score_db:.3f}',\n",
        "                             fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyser la coh√©rence entre m√©triques\n",
        "print(f\"\\nAnalyse de coh√©rence entre m√©triques:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, metrics in metrics_results.items():\n",
        "    k_range = metrics['k_range']\n",
        "\n",
        "    best_k_sil = k_range[np.argmax(metrics['silhouette'])]\n",
        "    best_k_cal = k_range[np.argmax(metrics['calinski'])]\n",
        "    best_k_db = k_range[np.argmin(metrics['davies_bouldin'])]\n",
        "\n",
        "    print(f\"\\n{dataset_name}:\")\n",
        "    print(f\"  Silhouette recommande K={best_k_sil}\")\n",
        "    print(f\"  Calinski-Harabasz recommande K={best_k_cal}\")\n",
        "    print(f\"  Davies-Bouldin recommande K={best_k_db}\")\n",
        "\n",
        "    # V√©rifier la coh√©rence\n",
        "    recommendations = [best_k_sil, best_k_cal, best_k_db]\n",
        "    if len(set(recommendations)) == 1:\n",
        "        print(f\"  ‚úÖ CONSENSUS: Toutes les m√©triques recommandent K={recommendations[0]}\")\n",
        "    elif len(set(recommendations)) == 2:\n",
        "        print(f\"  ‚ö†Ô∏è ACCORD PARTIEL: Pas de consensus complet\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå D√âSACCORD: M√©triques contradictoires\")\n",
        "\n",
        "# Guide d'interpr√©tation des m√©triques\n",
        "print(f\"\\nGuide d'interpr√©tation des m√©triques:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"SILHOUETTE SCORE (-1 √† +1):\")\n",
        "print(\"  > 0.7  : Excellente s√©paration\")\n",
        "print(\"  0.5-0.7: Bonne s√©paration\")\n",
        "print(\"  0.3-0.5: S√©paration correcte\")\n",
        "print(\"  < 0.3  : S√©paration faible\")\n",
        "print(\"  < 0   : Clustering incorrect\")\n",
        "\n",
        "print(\"\\nCALINSKI-HARABASZ INDEX (plus √©lev√© = mieux):\")\n",
        "print(\"  > 1000 : Tr√®s bons clusters\")\n",
        "print(\"  500-1000: Bons clusters\")\n",
        "print(\"  100-500: Clusters corrects\")\n",
        "print(\"  < 100  : Clusters faibles\")\n",
        "\n",
        "print(\"\\nDAVIES-BOULDIN INDEX (plus bas = mieux):\")\n",
        "print(\"  < 0.5  : Excellents clusters\")\n",
        "print(\"  0.5-1.0: Bons clusters\")\n",
        "print(\"  1.0-1.5: Clusters corrects\")\n",
        "print(\"  > 1.5  : Clusters faibles\")\n",
        "\n",
        "# Analyse des silhouettes individuelles\n",
        "print(f\"\\nAnalyse des silhouettes individuelles:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prendre le premier dataset pour l'analyse d√©taill√©e\n",
        "X_example, y_example = datasets['Facile (bien s√©par√©s)']\n",
        "scaler_example = StandardScaler()\n",
        "X_example_scaled = scaler_example.fit_transform(X_example)\n",
        "\n",
        "# Appliquer K-Means avec K=4\n",
        "kmeans_example = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "clusters_example = kmeans_example.fit_predict(X_example_scaled)\n",
        "\n",
        "# Calculer les silhouettes individuelles\n",
        "silhouette_individual = silhouette_samples(X_example_scaled, clusters_example)\n",
        "\n",
        "# Visualiser les silhouettes par cluster\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "y_lower = 10\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 4))\n",
        "\n",
        "for i in range(4):\n",
        "    cluster_silhouettes = silhouette_individual[clusters_example == i]\n",
        "    cluster_silhouettes.sort()\n",
        "\n",
        "    size_cluster_i = cluster_silhouettes.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouettes,\n",
        "                     facecolor=colors[i], edgecolor=colors[i], alpha=0.7)\n",
        "\n",
        "    # Ajouter le label au milieu du cluster\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontweight='bold')\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.xlabel('Valeur de Silhouette')\n",
        "plt.ylabel('Index des Clusters')\n",
        "plt.title('Silhouettes Individuelles par Cluster', fontweight='bold')\n",
        "\n",
        "# Ligne verticale pour la silhouette moyenne\n",
        "avg_silhouette = silhouette_score(X_example_scaled, clusters_example)\n",
        "plt.axvline(x=avg_silhouette, color=\"red\", linestyle=\"--\", linewidth=2,\n",
        "           label=f'Moyenne: {avg_silhouette:.3f}')\n",
        "plt.legend()\n",
        "\n",
        "# Scatter plot avec silhouettes color√©es\n",
        "plt.subplot(2, 2, 2)\n",
        "scatter = plt.scatter(X_example[:, 0], X_example[:, 1], c=silhouette_individual,\n",
        "                     cmap='RdYlBu', alpha=0.7, s=50)\n",
        "plt.colorbar(scatter, label='Silhouette individuelle')\n",
        "plt.title('Donn√©es color√©es par Silhouette', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Histogramme des silhouettes\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(silhouette_individual, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(avg_silhouette, color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Moyenne: {avg_silhouette:.3f}')\n",
        "plt.xlabel('Valeur de Silhouette')\n",
        "plt.ylabel('Nombre de Points')\n",
        "plt.title('Distribution des Silhouettes', fontweight='bold')\n",
        "plt.legend()\n",
        "\n",
        "# Analyser les points probl√©matiques\n",
        "problematic_points = silhouette_individual < 0\n",
        "n_problematic = problematic_points.sum()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "if n_problematic > 0:\n",
        "    plt.scatter(X_example[~problematic_points, 0], X_example[~problematic_points, 1],\n",
        "               c=clusters_example[~problematic_points], cmap='tab10', alpha=0.7, s=50,\n",
        "               label=f'Points OK ({(~problematic_points).sum()})')\n",
        "    plt.scatter(X_example[problematic_points, 0], X_example[problematic_points, 1],\n",
        "               c='red', marker='x', s=100, alpha=0.9,\n",
        "               label=f'Points probl√©matiques ({n_problematic})')\n",
        "else:\n",
        "    plt.scatter(X_example[:, 0], X_example[:, 1], c=clusters_example,\n",
        "               cmap='tab10', alpha=0.7, s=50)\n",
        "\n",
        "plt.title('Points Probl√©matiques (Silhouette < 0)', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAnalyse des points probl√©matiques:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Points avec silhouette n√©gative: {n_problematic} ({n_problematic/len(X_example)*100:.1f}%)\")\n",
        "\n",
        "if n_problematic > 0:\n",
        "    print(f\"Interpr√©tation:\")\n",
        "    print(f\"  ‚Ä¢ Ces points sont plus proches d'autres clusters que du leur\")\n",
        "    print(f\"  ‚Ä¢ Possible mauvaise assignation\")\n",
        "    print(f\"  ‚Ä¢ Candidats pour r√©assignation ou suppression\")\n",
        "\n",
        "    # Analyser par cluster\n",
        "    for cluster_id in range(4):\n",
        "        cluster_mask = clusters_example == cluster_id\n",
        "        cluster_problematic = problematic_points[cluster_mask].sum()\n",
        "        cluster_total = cluster_mask.sum()\n",
        "\n",
        "        if cluster_total > 0:\n",
        "            pct_problematic = cluster_problematic / cluster_total * 100\n",
        "            print(f\"  Cluster {cluster_id}: {cluster_problematic}/{cluster_total} probl√©matiques ({pct_problematic:.1f}%)\")\n",
        "\n",
        "# Comparer diff√©rents algorithmes de clustering\n",
        "print(f\"\\nComparaison d'algorithmes sur le dataset 'Difficile':\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X_test, y_test = datasets['Difficile (proches)']\n",
        "scaler_test = StandardScaler()\n",
        "X_test_scaled = scaler_test.fit_transform(X_test)\n",
        "\n",
        "algorithms = {\n",
        "    'K-Means (K=4)': KMeans(n_clusters=4, random_state=42, n_init=10),\n",
        "    'K-Means (K=3)': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "    'K-Means (K=5)': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
        "    'DBSCAN (eps=0.5)': DBSCAN(eps=0.5, min_samples=5)\n",
        "}\n",
        "\n",
        "algorithm_results = {}\n",
        "\n",
        "print(f\"{'Algorithme':<20} {'Silhouette':<12} {'Calinski':<12} {'Davies-Bouldin':<15} {'N_Clusters'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for algo_name, algo in algorithms.items():\n",
        "    clusters_algo = algo.fit_predict(X_test_scaled)\n",
        "\n",
        "    # Calculer le nombre de clusters (exclure le bruit pour DBSCAN)\n",
        "    n_clusters_algo = len(set(clusters_algo)) - (1 if -1 in clusters_algo else 0)\n",
        "\n",
        "    if n_clusters_algo > 1:\n",
        "        # Pour les m√©triques, exclure le bruit\n",
        "        if -1 in clusters_algo:\n",
        "            mask_no_noise = clusters_algo != -1\n",
        "            X_for_metrics = X_test_scaled[mask_no_noise]\n",
        "            clusters_for_metrics = clusters_algo[mask_no_noise]\n",
        "        else:\n",
        "            X_for_metrics = X_test_scaled\n",
        "            clusters_for_metrics = clusters_algo\n",
        "\n",
        "        if len(set(clusters_for_metrics)) > 1:\n",
        "            sil_algo = silhouette_score(X_for_metrics, clusters_for_metrics)\n",
        "            cal_algo = calinski_harabasz_score(X_for_metrics, clusters_for_metrics)\n",
        "            db_algo = davies_bouldin_score(X_for_metrics, clusters_for_metrics)\n",
        "        else:\n",
        "            sil_algo = cal_algo = db_algo = -999\n",
        "    else:\n",
        "        sil_algo = cal_algo = db_algo = -999\n",
        "\n",
        "    algorithm_results[algo_name] = {\n",
        "        'silhouette': sil_algo,\n",
        "        'calinski': cal_algo,\n",
        "        'davies_bouldin': db_algo,\n",
        "        'n_clusters': n_clusters_algo,\n",
        "        'clusters': clusters_algo\n",
        "    }\n",
        "\n",
        "    print(f\"{algo_name:<20} {sil_algo:<12.3f} {cal_algo:<12.1f} {db_algo:<15.3f} {n_clusters_algo}\")\n",
        "\n",
        "# Identifier le meilleur algorithme\n",
        "best_algorithm = max(algorithm_results.keys(),\n",
        "                    key=lambda k: algorithm_results[k]['silhouette'] if algorithm_results[k]['silhouette'] > -999 else -1)\n",
        "\n",
        "print(f\"\\nMeilleur algorithme: {best_algorithm}\")\n",
        "print(f\"  Silhouette: {algorithm_results[best_algorithm]['silhouette']:.3f}\")\n",
        "print(f\"  Clusters d√©tect√©s: {algorithm_results[best_algorithm]['n_clusters']}\")\n",
        "\n",
        "# Guide de choix de m√©trique\n",
        "print(f\"\\nGuide de choix de m√©trique d'√©valuation:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"UTILISEZ SILHOUETTE SCORE quand:\")\n",
        "print(\"  ‚Ä¢ Vous voulez une m√©trique intuitive et normalis√©e\")\n",
        "print(\"  ‚Ä¢ Vous comparez diff√©rents algorithmes\")\n",
        "print(\"  ‚Ä¢ Vous voulez d√©tecter les points mal class√©s\")\n",
        "print(\"  ‚Ä¢ Standard de l'industrie\")\n",
        "\n",
        "print(\"\\nUTILISEZ CALINSKI-HARABASZ quand:\")\n",
        "print(\"  ‚Ä¢ Vous avez des clusters de tailles tr√®s diff√©rentes\")\n",
        "print(\"  ‚Ä¢ Vous voulez favoriser la compacit√©\")\n",
        "print(\"  ‚Ä¢ Calcul plus rapide sur gros datasets\")\n",
        "\n",
        "print(\"\\nUTILISEZ DAVIES-BOULDIN quand:\")\n",
        "print(\"  ‚Ä¢ Vous voulez p√©naliser les clusters proches\")\n",
        "print(\"  ‚Ä¢ Vous pr√©f√©rez des clusters bien s√©par√©s\")\n",
        "print(\"  ‚Ä¢ Alternative au silhouette score\")\n",
        "\n",
        "# M√©triques externes (quand on conna√Æt la v√©rit√©)\n",
        "print(f\"\\nM√©triques externes (quand v√©rit√© terrain disponible):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
        "\n",
        "# Utiliser le meilleur clustering trouv√©\n",
        "best_clusters = algorithm_results[best_algorithm]['clusters']\n",
        "\n",
        "# Calculer les m√©triques externes\n",
        "ari = adjusted_rand_score(y_test, best_clusters)\n",
        "nmi = normalized_mutual_info_score(y_test, best_clusters)\n",
        "fmi = fowlkes_mallows_score(y_test, best_clusters)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n",
        "print(f\"  ‚Üí Mesure l'accord global entre clusterings\")\n",
        "print(f\"  ‚Üí 1.0 = accord parfait, 0.0 = accord al√©atoire\")\n",
        "\n",
        "print(f\"\\nNormalized Mutual Information: {nmi:.3f}\")\n",
        "print(f\"  ‚Üí Mesure l'information partag√©e\")\n",
        "print(f\"  ‚Üí 1.0 = information parfaitement partag√©e\")\n",
        "\n",
        "print(f\"\\nFowlkes-Mallows Index: {fmi:.3f}\")\n",
        "print(f\"  ‚Üí Moyenne g√©om√©trique de pr√©cision et rappel\")\n",
        "print(f\"  ‚Üí 1.0 = clustering parfait\")\n",
        "\n",
        "# Recommandations finales\n",
        "print(f\"\\nRecommandations pour √©valuer vos clusterings:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"1. COMMENCEZ par la visualisation\")\n",
        "print(\"   ‚Ä¢ Tracez vos donn√©es si possible (2D/3D)\")\n",
        "print(\"   ‚Ä¢ V√©rifiez visuellement la coh√©rence\")\n",
        "\n",
        "print(\"\\n2. UTILISEZ plusieurs m√©triques\")\n",
        "print(\"   ‚Ä¢ Silhouette score (standard)\")\n",
        "print(\"   ‚Ä¢ Plus une m√©trique compl√©mentaire\")\n",
        "print(\"   ‚Ä¢ Cherchez la coh√©rence entre m√©triques\")\n",
        "\n",
        "print(\"\\n3. VALIDEZ avec l'expertise m√©tier\")\n",
        "print(\"   ‚Ä¢ Les clusters ont-ils du sens business ?\")\n",
        "print(\"   ‚Ä¢ Sont-ils exploitables op√©rationnellement ?\")\n",
        "print(\"   ‚Ä¢ Apportent-ils des insights nouveaux ?\")\n",
        "\n",
        "print(\"\\n4. TESTEZ la stabilit√©\")\n",
        "print(\"   ‚Ä¢ Relancez l'algorithme plusieurs fois\")\n",
        "print(\"   ‚Ä¢ Testez sur des sous-√©chantillons\")\n",
        "print(\"   ‚Ä¢ V√©rifiez la robustesse aux param√®tres\")\n",
        "\n",
        "# Cas o√π les m√©triques sont contradictoires\n",
        "print(f\"\\nQue faire si les m√©triques se contredisent ?\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚Ä¢ Privil√©giez le silhouette score (plus fiable)\")\n",
        "print(\"‚Ä¢ Visualisez les r√©sultats pour comprendre\")\n",
        "print(\"‚Ä¢ Testez sur un sous-√©chantillon\")\n",
        "print(\"‚Ä¢ Consultez l'expertise m√©tier\")\n",
        "print(\"‚Ä¢ Consid√©rez que vos donn√©es n'ont peut-√™tre pas de structure claire\")\n",
        "\n",
        "print(f\"\\nüí° R√®gle d'or:\")\n",
        "print(\"Une bonne √©valuation combine m√©triques quantitatives,\")\n",
        "print(\"visualisation et validation m√©tier. Aucune m√©trique\")\n",
        "print(\"seule ne peut garantir la qualit√© d'un clustering !\")"
      ]
    }
  ]
}