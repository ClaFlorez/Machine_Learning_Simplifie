{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNypw1am8oJ9L/Hj++vqIU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaFlorez/Machine_Learning_Simplifie/blob/main/8_4_Evaluation_complete_du_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMSH036qaKbe"
      },
      "outputs": [],
      "source": [
        "#Évaluation complète du clustering\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Créer plusieurs datasets avec différentes qualités de clustering\n",
        "print(\"Évaluation de la qualité du clustering\")\n",
        "print(\"Comparaison sur différents types de données\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dataset 1: Clusters bien séparés (facile)\n",
        "X_easy, y_easy = make_blobs(n_samples=300, centers=4, n_features=2,\n",
        "                           cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Dataset 2: Clusters proches (difficile)\n",
        "X_hard, y_hard = make_blobs(n_samples=300, centers=4, n_features=2,\n",
        "                           cluster_std=2.5, random_state=42)\n",
        "\n",
        "# Dataset 3: Clusters avec densités différentes\n",
        "X_varied = []\n",
        "y_varied = []\n",
        "centers_varied = [[0, 0], [8, 8], [0, 8], [8, 0]]\n",
        "stds_varied = [0.5, 1.5, 1.0, 2.0]  # Densités différentes\n",
        "\n",
        "for i, (center, std) in enumerate(zip(centers_varied, stds_varied)):\n",
        "    n_points = [100, 50, 75, 75][i]  # Tailles différentes aussi\n",
        "    cluster_points = np.random.normal(center, std, (n_points, 2))\n",
        "    X_varied.extend(cluster_points)\n",
        "    y_varied.extend([i] * n_points)\n",
        "\n",
        "X_varied = np.array(X_varied)\n",
        "y_varied = np.array(y_varied)\n",
        "\n",
        "datasets = {\n",
        "    'Facile (bien séparés)': (X_easy, y_easy),\n",
        "    'Difficile (proches)': (X_hard, y_hard),\n",
        "    'Densités variées': (X_varied, y_varied)\n",
        "}\n",
        "\n",
        "print(f\"Trois datasets de test créés:\")\n",
        "for name, (X, y) in datasets.items():\n",
        "    print(f\"  {name}: {len(X)} points, {len(np.unique(y))} clusters vrais\")\n",
        "\n",
        "# Tester K-Means avec différents K sur chaque dataset\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "\n",
        "metrics_results = {}\n",
        "\n",
        "for dataset_idx, (dataset_name, (X, y_true)) in enumerate(datasets.items()):\n",
        "    print(f\"\\nAnalyse du dataset: {dataset_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Standardiser\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Tester différents nombres de clusters\n",
        "    k_range = range(2, 8)\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "\n",
        "    for k in k_range:\n",
        "        # Appliquer K-Means\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Calculer les métriques\n",
        "        sil_score = silhouette_score(X_scaled, clusters)\n",
        "        calinski_score = calinski_harabasz_score(X_scaled, clusters)\n",
        "        db_score = davies_bouldin_score(X_scaled, clusters)\n",
        "\n",
        "        silhouette_scores.append(sil_score)\n",
        "        calinski_scores.append(calinski_score)\n",
        "        davies_bouldin_scores.append(db_score)\n",
        "\n",
        "        print(f\"K={k}: Silhouette={sil_score:.3f}, Calinski={calinski_score:.1f}, Davies-Bouldin={db_score:.3f}\")\n",
        "\n",
        "    # Stocker les résultats\n",
        "    metrics_results[dataset_name] = {\n",
        "        'silhouette': silhouette_scores,\n",
        "        'calinski': calinski_scores,\n",
        "        'davies_bouldin': davies_bouldin_scores,\n",
        "        'k_range': list(k_range)\n",
        "    }\n",
        "\n",
        "    # Visualiser les données originales\n",
        "    axes[dataset_idx, 0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='tab10', alpha=0.7)\n",
        "    axes[dataset_idx, 0].set_title(f'{dataset_name}\\n(Vraies classes)', fontweight='bold')\n",
        "\n",
        "    # Trouver le K optimal selon chaque métrique\n",
        "    best_k_sil = k_range[np.argmax(silhouette_scores)]\n",
        "    best_k_cal = k_range[np.argmax(calinski_scores)]\n",
        "    best_k_db = k_range[np.argmin(davies_bouldin_scores)]  # Davies-Bouldin: plus bas = mieux\n",
        "\n",
        "    print(f\"K optimal selon Silhouette: {best_k_sil}\")\n",
        "    print(f\"K optimal selon Calinski-Harabasz: {best_k_cal}\")\n",
        "    print(f\"K optimal selon Davies-Bouldin: {best_k_db}\")\n",
        "\n",
        "    # Visualiser le clustering avec K optimal (silhouette)\n",
        "    kmeans_optimal = KMeans(n_clusters=best_k_sil, random_state=42, n_init=10)\n",
        "    clusters_optimal = kmeans_optimal.fit_predict(X_scaled)\n",
        "\n",
        "    axes[dataset_idx, 1].scatter(X[:, 0], X[:, 1], c=clusters_optimal, cmap='tab10', alpha=0.7)\n",
        "    axes[dataset_idx, 1].scatter(kmeans_optimal.cluster_centers_[:, 0], kmeans_optimal.cluster_centers_[:, 1],\n",
        "                                c='red', marker='x', s=200, linewidths=3)\n",
        "    axes[dataset_idx, 1].set_title(f'K-Means (K={best_k_sil})\\nSilhouette={silhouette_scores[best_k_sil-2]:.3f}',\n",
        "                                  fontweight='bold')\n",
        "\n",
        "# Graphiques de métriques\n",
        "for dataset_idx, (dataset_name, metrics) in enumerate(metrics_results.items()):\n",
        "    k_range = metrics['k_range']\n",
        "\n",
        "    # Silhouette scores\n",
        "    axes[dataset_idx, 2].plot(k_range, metrics['silhouette'], 'o-', linewidth=2, markersize=8)\n",
        "    axes[dataset_idx, 2].set_title(f'Silhouette Score\\n{dataset_name}', fontweight='bold')\n",
        "    axes[dataset_idx, 2].set_xlabel('Nombre de Clusters (K)')\n",
        "    axes[dataset_idx, 2].set_ylabel('Silhouette Score')\n",
        "    axes[dataset_idx, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Marquer le maximum\n",
        "    best_idx = np.argmax(metrics['silhouette'])\n",
        "    best_k = k_range[best_idx]\n",
        "    best_score = metrics['silhouette'][best_idx]\n",
        "    axes[dataset_idx, 2].axvline(best_k, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[dataset_idx, 2].text(best_k + 0.1, best_score, f'Max: {best_score:.3f}',\n",
        "                             fontweight='bold')\n",
        "\n",
        "    # Davies-Bouldin (plus bas = mieux)\n",
        "    axes[dataset_idx, 3].plot(k_range, metrics['davies_bouldin'], 's-', linewidth=2, markersize=8, color='red')\n",
        "    axes[dataset_idx, 3].set_title(f'Davies-Bouldin Index\\n{dataset_name}', fontweight='bold')\n",
        "    axes[dataset_idx, 3].set_xlabel('Nombre de Clusters (K)')\n",
        "    axes[dataset_idx, 3].set_ylabel('Davies-Bouldin Index')\n",
        "    axes[dataset_idx, 3].grid(True, alpha=0.3)\n",
        "\n",
        "    # Marquer le minimum\n",
        "    best_idx_db = np.argmin(metrics['davies_bouldin'])\n",
        "    best_k_db = k_range[best_idx_db]\n",
        "    best_score_db = metrics['davies_bouldin'][best_idx_db]\n",
        "    axes[dataset_idx, 3].axvline(best_k_db, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[dataset_idx, 3].text(best_k_db + 0.1, best_score_db, f'Min: {best_score_db:.3f}',\n",
        "                             fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyser la cohérence entre métriques\n",
        "print(f\"\\nAnalyse de cohérence entre métriques:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, metrics in metrics_results.items():\n",
        "    k_range = metrics['k_range']\n",
        "\n",
        "    best_k_sil = k_range[np.argmax(metrics['silhouette'])]\n",
        "    best_k_cal = k_range[np.argmax(metrics['calinski'])]\n",
        "    best_k_db = k_range[np.argmin(metrics['davies_bouldin'])]\n",
        "\n",
        "    print(f\"\\n{dataset_name}:\")\n",
        "    print(f\"  Silhouette recommande K={best_k_sil}\")\n",
        "    print(f\"  Calinski-Harabasz recommande K={best_k_cal}\")\n",
        "    print(f\"  Davies-Bouldin recommande K={best_k_db}\")\n",
        "\n",
        "    # Vérifier la cohérence\n",
        "    recommendations = [best_k_sil, best_k_cal, best_k_db]\n",
        "    if len(set(recommendations)) == 1:\n",
        "        print(f\"  ✅ CONSENSUS: Toutes les métriques recommandent K={recommendations[0]}\")\n",
        "    elif len(set(recommendations)) == 2:\n",
        "        print(f\"  ⚠️ ACCORD PARTIEL: Pas de consensus complet\")\n",
        "    else:\n",
        "        print(f\"  ❌ DÉSACCORD: Métriques contradictoires\")\n",
        "\n",
        "# Guide d'interprétation des métriques\n",
        "print(f\"\\nGuide d'interprétation des métriques:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"SILHOUETTE SCORE (-1 à +1):\")\n",
        "print(\"  > 0.7  : Excellente séparation\")\n",
        "print(\"  0.5-0.7: Bonne séparation\")\n",
        "print(\"  0.3-0.5: Séparation correcte\")\n",
        "print(\"  < 0.3  : Séparation faible\")\n",
        "print(\"  < 0   : Clustering incorrect\")\n",
        "\n",
        "print(\"\\nCALINSKI-HARABASZ INDEX (plus élevé = mieux):\")\n",
        "print(\"  > 1000 : Très bons clusters\")\n",
        "print(\"  500-1000: Bons clusters\")\n",
        "print(\"  100-500: Clusters corrects\")\n",
        "print(\"  < 100  : Clusters faibles\")\n",
        "\n",
        "print(\"\\nDAVIES-BOULDIN INDEX (plus bas = mieux):\")\n",
        "print(\"  < 0.5  : Excellents clusters\")\n",
        "print(\"  0.5-1.0: Bons clusters\")\n",
        "print(\"  1.0-1.5: Clusters corrects\")\n",
        "print(\"  > 1.5  : Clusters faibles\")\n",
        "\n",
        "# Analyse des silhouettes individuelles\n",
        "print(f\"\\nAnalyse des silhouettes individuelles:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prendre le premier dataset pour l'analyse détaillée\n",
        "X_example, y_example = datasets['Facile (bien séparés)']\n",
        "scaler_example = StandardScaler()\n",
        "X_example_scaled = scaler_example.fit_transform(X_example)\n",
        "\n",
        "# Appliquer K-Means avec K=4\n",
        "kmeans_example = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "clusters_example = kmeans_example.fit_predict(X_example_scaled)\n",
        "\n",
        "# Calculer les silhouettes individuelles\n",
        "silhouette_individual = silhouette_samples(X_example_scaled, clusters_example)\n",
        "\n",
        "# Visualiser les silhouettes par cluster\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "y_lower = 10\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 4))\n",
        "\n",
        "for i in range(4):\n",
        "    cluster_silhouettes = silhouette_individual[clusters_example == i]\n",
        "    cluster_silhouettes.sort()\n",
        "\n",
        "    size_cluster_i = cluster_silhouettes.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouettes,\n",
        "                     facecolor=colors[i], edgecolor=colors[i], alpha=0.7)\n",
        "\n",
        "    # Ajouter le label au milieu du cluster\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontweight='bold')\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.xlabel('Valeur de Silhouette')\n",
        "plt.ylabel('Index des Clusters')\n",
        "plt.title('Silhouettes Individuelles par Cluster', fontweight='bold')\n",
        "\n",
        "# Ligne verticale pour la silhouette moyenne\n",
        "avg_silhouette = silhouette_score(X_example_scaled, clusters_example)\n",
        "plt.axvline(x=avg_silhouette, color=\"red\", linestyle=\"--\", linewidth=2,\n",
        "           label=f'Moyenne: {avg_silhouette:.3f}')\n",
        "plt.legend()\n",
        "\n",
        "# Scatter plot avec silhouettes colorées\n",
        "plt.subplot(2, 2, 2)\n",
        "scatter = plt.scatter(X_example[:, 0], X_example[:, 1], c=silhouette_individual,\n",
        "                     cmap='RdYlBu', alpha=0.7, s=50)\n",
        "plt.colorbar(scatter, label='Silhouette individuelle')\n",
        "plt.title('Données colorées par Silhouette', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Histogramme des silhouettes\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(silhouette_individual, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(avg_silhouette, color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Moyenne: {avg_silhouette:.3f}')\n",
        "plt.xlabel('Valeur de Silhouette')\n",
        "plt.ylabel('Nombre de Points')\n",
        "plt.title('Distribution des Silhouettes', fontweight='bold')\n",
        "plt.legend()\n",
        "\n",
        "# Analyser les points problématiques\n",
        "problematic_points = silhouette_individual < 0\n",
        "n_problematic = problematic_points.sum()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "if n_problematic > 0:\n",
        "    plt.scatter(X_example[~problematic_points, 0], X_example[~problematic_points, 1],\n",
        "               c=clusters_example[~problematic_points], cmap='tab10', alpha=0.7, s=50,\n",
        "               label=f'Points OK ({(~problematic_points).sum()})')\n",
        "    plt.scatter(X_example[problematic_points, 0], X_example[problematic_points, 1],\n",
        "               c='red', marker='x', s=100, alpha=0.9,\n",
        "               label=f'Points problématiques ({n_problematic})')\n",
        "else:\n",
        "    plt.scatter(X_example[:, 0], X_example[:, 1], c=clusters_example,\n",
        "               cmap='tab10', alpha=0.7, s=50)\n",
        "\n",
        "plt.title('Points Problématiques (Silhouette < 0)', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAnalyse des points problématiques:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Points avec silhouette négative: {n_problematic} ({n_problematic/len(X_example)*100:.1f}%)\")\n",
        "\n",
        "if n_problematic > 0:\n",
        "    print(f\"Interprétation:\")\n",
        "    print(f\"  • Ces points sont plus proches d'autres clusters que du leur\")\n",
        "    print(f\"  • Possible mauvaise assignation\")\n",
        "    print(f\"  • Candidats pour réassignation ou suppression\")\n",
        "\n",
        "    # Analyser par cluster\n",
        "    for cluster_id in range(4):\n",
        "        cluster_mask = clusters_example == cluster_id\n",
        "        cluster_problematic = problematic_points[cluster_mask].sum()\n",
        "        cluster_total = cluster_mask.sum()\n",
        "\n",
        "        if cluster_total > 0:\n",
        "            pct_problematic = cluster_problematic / cluster_total * 100\n",
        "            print(f\"  Cluster {cluster_id}: {cluster_problematic}/{cluster_total} problématiques ({pct_problematic:.1f}%)\")\n",
        "\n",
        "# Comparer différents algorithmes de clustering\n",
        "print(f\"\\nComparaison d'algorithmes sur le dataset 'Difficile':\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X_test, y_test = datasets['Difficile (proches)']\n",
        "scaler_test = StandardScaler()\n",
        "X_test_scaled = scaler_test.fit_transform(X_test)\n",
        "\n",
        "algorithms = {\n",
        "    'K-Means (K=4)': KMeans(n_clusters=4, random_state=42, n_init=10),\n",
        "    'K-Means (K=3)': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "    'K-Means (K=5)': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
        "    'DBSCAN (eps=0.5)': DBSCAN(eps=0.5, min_samples=5)\n",
        "}\n",
        "\n",
        "algorithm_results = {}\n",
        "\n",
        "print(f\"{'Algorithme':<20} {'Silhouette':<12} {'Calinski':<12} {'Davies-Bouldin':<15} {'N_Clusters'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for algo_name, algo in algorithms.items():\n",
        "    clusters_algo = algo.fit_predict(X_test_scaled)\n",
        "\n",
        "    # Calculer le nombre de clusters (exclure le bruit pour DBSCAN)\n",
        "    n_clusters_algo = len(set(clusters_algo)) - (1 if -1 in clusters_algo else 0)\n",
        "\n",
        "    if n_clusters_algo > 1:\n",
        "        # Pour les métriques, exclure le bruit\n",
        "        if -1 in clusters_algo:\n",
        "            mask_no_noise = clusters_algo != -1\n",
        "            X_for_metrics = X_test_scaled[mask_no_noise]\n",
        "            clusters_for_metrics = clusters_algo[mask_no_noise]\n",
        "        else:\n",
        "            X_for_metrics = X_test_scaled\n",
        "            clusters_for_metrics = clusters_algo\n",
        "\n",
        "        if len(set(clusters_for_metrics)) > 1:\n",
        "            sil_algo = silhouette_score(X_for_metrics, clusters_for_metrics)\n",
        "            cal_algo = calinski_harabasz_score(X_for_metrics, clusters_for_metrics)\n",
        "            db_algo = davies_bouldin_score(X_for_metrics, clusters_for_metrics)\n",
        "        else:\n",
        "            sil_algo = cal_algo = db_algo = -999\n",
        "    else:\n",
        "        sil_algo = cal_algo = db_algo = -999\n",
        "\n",
        "    algorithm_results[algo_name] = {\n",
        "        'silhouette': sil_algo,\n",
        "        'calinski': cal_algo,\n",
        "        'davies_bouldin': db_algo,\n",
        "        'n_clusters': n_clusters_algo,\n",
        "        'clusters': clusters_algo\n",
        "    }\n",
        "\n",
        "    print(f\"{algo_name:<20} {sil_algo:<12.3f} {cal_algo:<12.1f} {db_algo:<15.3f} {n_clusters_algo}\")\n",
        "\n",
        "# Identifier le meilleur algorithme\n",
        "best_algorithm = max(algorithm_results.keys(),\n",
        "                    key=lambda k: algorithm_results[k]['silhouette'] if algorithm_results[k]['silhouette'] > -999 else -1)\n",
        "\n",
        "print(f\"\\nMeilleur algorithme: {best_algorithm}\")\n",
        "print(f\"  Silhouette: {algorithm_results[best_algorithm]['silhouette']:.3f}\")\n",
        "print(f\"  Clusters détectés: {algorithm_results[best_algorithm]['n_clusters']}\")\n",
        "\n",
        "# Guide de choix de métrique\n",
        "print(f\"\\nGuide de choix de métrique d'évaluation:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"UTILISEZ SILHOUETTE SCORE quand:\")\n",
        "print(\"  • Vous voulez une métrique intuitive et normalisée\")\n",
        "print(\"  • Vous comparez différents algorithmes\")\n",
        "print(\"  • Vous voulez détecter les points mal classés\")\n",
        "print(\"  • Standard de l'industrie\")\n",
        "\n",
        "print(\"\\nUTILISEZ CALINSKI-HARABASZ quand:\")\n",
        "print(\"  • Vous avez des clusters de tailles très différentes\")\n",
        "print(\"  • Vous voulez favoriser la compacité\")\n",
        "print(\"  • Calcul plus rapide sur gros datasets\")\n",
        "\n",
        "print(\"\\nUTILISEZ DAVIES-BOULDIN quand:\")\n",
        "print(\"  • Vous voulez pénaliser les clusters proches\")\n",
        "print(\"  • Vous préférez des clusters bien séparés\")\n",
        "print(\"  • Alternative au silhouette score\")\n",
        "\n",
        "# Métriques externes (quand on connaît la vérité)\n",
        "print(f\"\\nMétriques externes (quand vérité terrain disponible):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
        "\n",
        "# Utiliser le meilleur clustering trouvé\n",
        "best_clusters = algorithm_results[best_algorithm]['clusters']\n",
        "\n",
        "# Calculer les métriques externes\n",
        "ari = adjusted_rand_score(y_test, best_clusters)\n",
        "nmi = normalized_mutual_info_score(y_test, best_clusters)\n",
        "fmi = fowlkes_mallows_score(y_test, best_clusters)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n",
        "print(f\"  → Mesure l'accord global entre clusterings\")\n",
        "print(f\"  → 1.0 = accord parfait, 0.0 = accord aléatoire\")\n",
        "\n",
        "print(f\"\\nNormalized Mutual Information: {nmi:.3f}\")\n",
        "print(f\"  → Mesure l'information partagée\")\n",
        "print(f\"  → 1.0 = information parfaitement partagée\")\n",
        "\n",
        "print(f\"\\nFowlkes-Mallows Index: {fmi:.3f}\")\n",
        "print(f\"  → Moyenne géométrique de précision et rappel\")\n",
        "print(f\"  → 1.0 = clustering parfait\")\n",
        "\n",
        "# Recommandations finales\n",
        "print(f\"\\nRecommandations pour évaluer vos clusterings:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"1. COMMENCEZ par la visualisation\")\n",
        "print(\"   • Tracez vos données si possible (2D/3D)\")\n",
        "print(\"   • Vérifiez visuellement la cohérence\")\n",
        "\n",
        "print(\"\\n2. UTILISEZ plusieurs métriques\")\n",
        "print(\"   • Silhouette score (standard)\")\n",
        "print(\"   • Plus une métrique complémentaire\")\n",
        "print(\"   • Cherchez la cohérence entre métriques\")\n",
        "\n",
        "print(\"\\n3. VALIDEZ avec l'expertise métier\")\n",
        "print(\"   • Les clusters ont-ils du sens business ?\")\n",
        "print(\"   • Sont-ils exploitables opérationnellement ?\")\n",
        "print(\"   • Apportent-ils des insights nouveaux ?\")\n",
        "\n",
        "print(\"\\n4. TESTEZ la stabilité\")\n",
        "print(\"   • Relancez l'algorithme plusieurs fois\")\n",
        "print(\"   • Testez sur des sous-échantillons\")\n",
        "print(\"   • Vérifiez la robustesse aux paramètres\")\n",
        "\n",
        "# Cas où les métriques sont contradictoires\n",
        "print(f\"\\nQue faire si les métriques se contredisent ?\")\n",
        "print(\"=\" * 50)\n",
        "print(\"• Privilégiez le silhouette score (plus fiable)\")\n",
        "print(\"• Visualisez les résultats pour comprendre\")\n",
        "print(\"• Testez sur un sous-échantillon\")\n",
        "print(\"• Consultez l'expertise métier\")\n",
        "print(\"• Considérez que vos données n'ont peut-être pas de structure claire\")\n",
        "\n",
        "print(f\"\\n💡 Règle d'or:\")\n",
        "print(\"Une bonne évaluation combine métriques quantitatives,\")\n",
        "print(\"visualisation et validation métier. Aucune métrique\")\n",
        "print(\"seule ne peut garantir la qualité d'un clustering !\")"
      ]
    }
  ]
}