# Strat√©gies d‚Äôam√©lioration par contexte

Ce m√©mo te donne **quoi faire selon le sympt√¥me** observ√© lors de l‚Äôentra√Ænement d‚Äôun mod√®le. Inclut un tableau r√©capitulatif, des *recipes* concr√®tes et des snippets scikit‚Äëlearn pour diagnostiquer rapidement.

## Tableau r√©capitulatif

| Situation                 | Sympt√¥me                               | Solutions recommand√©es |
|--------------------------|----------------------------------------|------------------------|
| **Overfitting**          | √âcart train/test > 10%                 | R√©gularisation (L1/L2), limiter la complexit√© (max_depth, n_estimators, C), **plus de donn√©es**, **feature selection**, **early stopping**, **subsample** (pour GBM) |
| **Underfitting**         | Scores faibles partout                 | Mod√®le **plus complexe** (RF/GBM/SVM), **feature engineering**, augmenter la capacit√© (plus d‚Äôestimateurs, profondeur), enlever/relaxer la r√©gularisation |
| **Donn√©es insuffisantes**| Variance √©lev√©e                        | **Data augmentation**, **collecte de donn√©es**, mod√®les simples, r√©gularisation, validation robuste (KFold stratifi√©) |
| **Features peu informatives** | Tous mod√®les m√©diocres            | **Feature engineering**, **feature selection** (mutual_info, RFECV), expertise m√©tier, nouvelles sources de donn√©es |
| **Classes d√©s√©quilibr√©es**| Accuracy √©lev√©e, **Recall** faible    | **R√©√©chantillonnage** (SMOTE/undersampling), **class_weight='balanced'**, m√©triques **AUC‚ÄëPR/Recall**, **seuil optimal** (tuning du threshold) |
| **Plateau de performance**| Pas d‚Äôam√©lioration                    | **Ensemble methods** (stacking/boosting), feature engineering, **plus de donn√©es**, *random search* / recherche bay√©sienne, calibration, tuning fin (learning_rate, max_depth) |

> üí° **Rappels rapides** : privil√©gie **AUC‚ÄëPR** et **Recall** sur classes rares; ne touche **jamais** au *test set* avant la fin; conserve une **baseline** de r√©f√©rence.

---

## Points cl√©s √† retenir

- **Approche syst√©matique** : *Baseline ‚Üí Tuning ‚Üí Feature Engineering ‚Üí Ensemble*  
- **Random Search** souvent meilleur (plus efficace que Grid Search sur espace large)  
- **Feature Engineering** : souvent plus d‚Äôimpact que le tuning pur  
- **Ensemble methods** : gain final, mais complexit√©/co√ªt accrus  
- **Validation rigoureuse** : *test final* intouch√© jusqu‚Äôau dernier moment  
- **√âquilibre performance/complexit√©** : ¬´ plus ¬ª n‚Äôest pas toujours ¬´ mieux ¬ª  

---

## Diagnostics rapides (scikit‚Äëlearn)

### 1) D√©tecter overfitting/underfitting
```python
from sklearn.model_selection import cross_val_score
import numpy as np

def gap_train_cv(model, X_train, y_train, scoring="accuracy", cv=5):
    model.fit(X_train, y_train)
    train_score = getattr(model, "score")(X_train, y_train)  # ou m√©trique custom
    cv_scores = cross_val_score(model, X_train, y_train, scoring=scoring, cv=cv)
    gap = train_score - cv_scores.mean()
    return {"train": train_score, "cv_mean": cv_scores.mean(), "cv_std": cv_scores.std(), "gap": gap}

# usage :
# from sklearn.ensemble import RandomForestClassifier
# res = gap_train_cv(RandomForestClassifier(random_state=42), X_train, y_train)
# print(res)  # gap>0.1 ‚âà overfitting, scores bas ‚âà underfitting
```

### 2) D√©s√©quilibre de classes
```python
import numpy as np
def ratio_classes(y):
    uniq, counts = np.unique(y, return_counts=True)
    ratios = counts / counts.sum()
    return dict(zip(uniq, ratios))
# if max(ratios.values()) > 0.8 ‚Üí fort d√©s√©quilibre
```

### 3) Seuil optimal pour maximiser le Recall sous contrainte de pr√©cision
```python
from sklearn.metrics import precision_recall_curve

def seuil_pour_recall(y_true, y_prob, precision_min=0.5):
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    mask = prec >= precision_min
    if mask.any():
        i = rec[mask].argmax()
        return float(thr[mask][i]), float(prec[mask][i]), float(rec[mask][i])
    return 0.5, float(prec.max()), float(rec.max())
```

### 4) S√©lection de features (RFECV)
```python
from sklearn.feature_selection import RFECV

def rfecv_features(estimator, X, y, scoring="accuracy", cv=5):
    selector = RFECV(estimator, step=1, cv=cv, scoring=scoring, n_jobs=-1)
    selector.fit(X, y)
    support = selector.support_  # bool par colonne
    return support, selector.ranking_, selector.n_features_
```

### 5) Random Search (tuning efficace)
```python
from sklearn.model_selection import RandomizedSearchCV

def random_search(model, param_distributions, X, y, scoring="accuracy", cv=5, n_iter=30):
    rs = RandomizedSearchCV(model, param_distributions=param_distributions, n_iter=n_iter, scoring=scoring, cv=cv, n_jobs=-1, random_state=42)
    rs.fit(X, y)
    return rs.best_estimator_, rs.best_params_, rs.best_score_
```

---

## Recettes rapides par cas

**Overfitting**  
- Limiter capacit√©: `max_depth`, `min_samples_leaf`, `max_features` (arbres/RF).  
- R√©gulariser: `C` (LogReg/SVM), `alpha` (Ridge/Lasso), `l2_regularization`.  
- Early stopping: `n_iter_no_change`/`early_stopping=True` (GBM/LogReg SAG/SAGA).  
- Sous‚Äë√©chantillonnage al√©atoire (`subsample<1.0`) en gradient boosting.

**Underfitting**  
- Plus de capacit√©: plus d‚Äôarbres/feuilles, `max_depth`‚Üë, `n_estimators`‚Üë.  
- Mod√®les non lin√©aires: RandomForest, GradientBoosting, XGBoost, SVM (RBF).

**Classes d√©s√©quilibr√©es**  
- `class_weight='balanced'` (LogReg/SVM/Tree).  
- SMOTE/ADASYN (imblearn), undersampling.  
- Reporter **AUC‚ÄëPR**, **Recall**, **PR‚Äëcurve**; r√©gler le seuil.

**Features peu informatives**  
- `mutual_info_classif/regression`, permutation importance, SHAP.  
- RFECV + cr√©ation de variables (ratios, interactions, encodages cibl√©s).

**Plateau de performance**  
- Stacking/Blending, GBM (learning_rate bas, +estimators), recherche bay√©sienne.  
- Plus de donn√©es et **meilleure** qualit√© d‚Äô√©tiquettes.

---

### Garde‚Äëfou final
- G√®le le **test set**.  
- Conserve un **notebook de tra√ßabilit√©** (versions, seeds, splits).  
- Sauvegarde **mod√®le + m√©tadonn√©es** (cf. fonction `save_model_and_metadata`).

