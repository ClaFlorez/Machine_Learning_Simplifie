{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdcocNo6l/Tq61lWiF4ILJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaFlorez/Machine_Learning_Simplifie/blob/main/11_1_Naive_Bayes_pour_classification_de_texte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT1xxUxwltuv",
        "outputId": "2bbbc5cf-cebe-4767-8302-264843936484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAIVE BAYES - CLASSIFICATION DE TEXTE\n",
            "Objectif: Classifier automatiquement des articles de news\n",
            "======================================================================\n",
            "Catégories à classifier: ['tech', 'sport', 'politique', 'economie']\n",
            "Dataset de texte créé:\n",
            "  • Nombre de textes: 400\n",
            "  • Catégories: 4\n",
            "  • tech: 100 textes (25.0%)\n",
            "  • sport: 100 textes (25.0%)\n",
            "  • politique: 100 textes (25.0%)\n",
            "  • economie: 100 textes (25.0%)\n",
            "\n",
            "Vectorisation TF-IDF des textes:\n",
            "========================================\n",
            "  • Vocabulaire: 419 mots\n",
            "  • Matrice TF-IDF: (400, 419)\n",
            "  • Densité: 1.6%\n",
            "\n",
            "Performance Naive Bayes sur texte:\n",
            "  • Accuracy: 1.0000 (100.0%)\n",
            "\n",
            "Rapport de classification détaillé:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    economie       1.00      1.00      1.00        20\n",
            "   politique       1.00      1.00      1.00        20\n",
            "       sport       1.00      1.00      1.00        20\n",
            "        tech       1.00      1.00      1.00        20\n",
            "\n",
            "    accuracy                           1.00        80\n",
            "   macro avg       1.00      1.00      1.00        80\n",
            "weighted avg       1.00      1.00      1.00        80\n",
            "\n",
            "\n",
            "Mots les plus discriminants par catégorie:\n",
            "============================================================\n",
            "\n",
            "ECONOMIE:\n",
            "  • import: -4.587\n",
            "  • commerce: -4.628\n",
            "  • export: -4.648\n",
            "  • international: -4.651\n",
            "  • affaires: -4.651\n",
            "  • banque: -4.661\n",
            "  • taux: -4.670\n",
            "  • crédit: -4.689\n",
            "  • chiffre: -4.696\n",
            "  • entreprise: -4.701\n",
            "\n",
            "POLITIQUE:\n",
            "  • discours: -4.499\n",
            "  • président: -4.534\n",
            "  • déclaration: -4.596\n",
            "  • parti: -4.597\n",
            "  • opposition: -4.598\n",
            "  • majorité: -4.639\n",
            "  • alliance: -4.656\n",
            "  • débat: -4.674\n",
            "  • loi: -4.679\n",
            "  • adoption: -4.680\n",
            "\n",
            "SPORT:\n",
            "  • natation: -4.571\n",
            "  • basketball: -4.576\n",
            "  • finale: -4.576\n",
            "  • chronométrage: -4.610\n",
            "  • nba: -4.637\n",
            "  • sprint: -4.669\n",
            "  • tennis: -4.673\n",
            "  • joueur: -4.717\n",
            "  • victoire: -4.723\n",
            "  • course: -4.727\n",
            "\n",
            "TECH:\n",
            "  • cybersécurité: -4.540\n",
            "  • automatisation: -4.553\n",
            "  • données: -4.561\n",
            "  • ordinateur: -4.570\n",
            "  • industrie: -4.574\n",
            "  • robotique: -4.576\n",
            "  • calcul: -4.579\n",
            "  • processeur: -4.613\n",
            "  • artificielle: -4.618\n",
            "  • futur: -4.629\n",
            "\n",
            "Test sur de nouveaux textes:\n",
            "========================================\n",
            "\n",
            "Texte 1: 'algorithme apprentissage automatique intelligence'\n",
            "  • Prédiction: tech\n",
            "  • Confiance: 78.3%\n",
            "  • Probabilités par classe:\n",
            "    - economie: 7.3%\n",
            "    - politique: 7.2%\n",
            "    - sport: 7.2%\n",
            "    - tech: 78.3%\n",
            "\n",
            "Texte 2: 'match football équipe victoire'\n",
            "  • Prédiction: sport\n",
            "  • Confiance: 88.3%\n",
            "  • Probabilités par classe:\n",
            "    - economie: 3.9%\n",
            "    - politique: 3.9%\n",
            "    - sport: 88.3%\n",
            "    - tech: 3.9%\n",
            "\n",
            "Texte 3: 'élection président vote démocratie'\n",
            "  • Prédiction: politique\n",
            "  • Confiance: 90.4%\n",
            "  • Probabilités par classe:\n",
            "    - economie: 3.2%\n",
            "    - politique: 90.4%\n",
            "    - sport: 3.2%\n",
            "    - tech: 3.2%\n",
            "\n",
            "Texte 4: 'entreprise bourse investissement profit'\n",
            "  • Prédiction: economie\n",
            "  • Confiance: 78.7%\n",
            "  • Probabilités par classe:\n",
            "    - economie: 78.7%\n",
            "    - politique: 7.1%\n",
            "    - sport: 7.1%\n",
            "    - tech: 7.1%\n"
          ]
        }
      ],
      "source": [
        "#Naive Bayes pour classification de texte\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from sklearn.metrics import classification_report # Import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Démonstration Naive Bayes sur classification de texte\n",
        "print(\"NAIVE BAYES - CLASSIFICATION DE TEXTE\")\n",
        "print(\"Objectif: Classifier automatiquement des articles de news\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Simuler un dataset de classification d'emails/articles\n",
        "categories = ['tech', 'sport', 'politique', 'economie']\n",
        "print(f\"Catégories à classifier: {categories}\")\n",
        "\n",
        "# Simuler des textes (en réalité, vous utiliseriez de vrais articles)\n",
        "textes_exemples = {\n",
        "    'tech': [\n",
        "        \"intelligence artificielle machine learning algorithme\",\n",
        "        \"smartphone application mobile technologie innovation\",\n",
        "        \"ordinateur processeur performance calcul\",\n",
        "        \"internet cybersécurité données protection\",\n",
        "        \"robotique automatisation industrie futur\"\n",
        "    ],\n",
        "    'sport': [\n",
        "        \"football match équipe victoire championnat\",\n",
        "        \"tennis tournoi joueur performance classement\",\n",
        "        \"basketball NBA finale playoffs\",\n",
        "        \"cyclisme course étape montagne sprint\",\n",
        "        \"natation piscine record chronométrage\"\n",
        "    ],\n",
        "    'politique': [\n",
        "        \"gouvernement ministre décision politique réforme\",\n",
        "        \"élection candidat vote campagne démocratie\",\n",
        "        \"parlement débat loi amendement adoption\",\n",
        "        \"président déclaration discours nation\",\n",
        "        \"parti opposition majorité alliance\"\n",
        "    ],\n",
        "    'economie': [\n",
        "        \"entreprise bénéfice chiffre affaires croissance\",\n",
        "        \"bourse action investissement marché financier\",\n",
        "        \"banque crédit taux intérêt inflation\",\n",
        "        \"commerce international export import\",\n",
        "        \"emploi chômage salaire économie travail\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Créer le dataset d'entraînement\n",
        "textes_train = []\n",
        "labels_train = []\n",
        "\n",
        "for categorie, exemples in textes_exemples.items():\n",
        "    for exemple in exemples:\n",
        "        # Répéter chaque exemple avec variations\n",
        "        for _ in range(20):  # 20 variations par exemple\n",
        "            # Ajouter du bruit et des variations\n",
        "            mots = exemple.split()\n",
        "            np.random.shuffle(mots)\n",
        "            texte_varie = ' '.join(mots[:np.random.randint(3, len(mots)+1)])\n",
        "\n",
        "            textes_train.append(texte_varie)\n",
        "            labels_train.append(categorie)\n",
        "\n",
        "print(f\"Dataset de texte créé:\")\n",
        "print(f\"  • Nombre de textes: {len(textes_train)}\")\n",
        "print(f\"  • Catégories: {len(set(labels_train))}\")\n",
        "\n",
        "# Distribution des classes\n",
        "for cat in categories:\n",
        "    count = labels_train.count(cat)\n",
        "    print(f\"  • {cat}: {count} textes ({count/len(labels_train)*100:.1f}%)\")\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "print(f\"\\nVectorisation TF-IDF des textes:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,     # Limiter le vocabulaire\n",
        "    stop_words='english',  # Supprimer les mots vides\n",
        "    ngram_range=(1, 2)     # Mots simples et bigrammes\n",
        ")\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(textes_train)\n",
        "print(f\"  • Vocabulaire: {len(vectorizer.get_feature_names_out())} mots\")\n",
        "print(f\"  • Matrice TF-IDF: {X_tfidf.shape}\")\n",
        "print(f\"  • Densité: {X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1]):.1%}\")\n",
        "\n",
        "# Diviser les données\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    X_tfidf, labels_train, test_size=0.2, random_state=42, stratify=labels_train\n",
        ")\n",
        "\n",
        "# Entraîner Naive Bayes Multinomial (optimal pour texte)\n",
        "nb_model = MultinomialNB(alpha=1.0)  # Lissage de Laplace\n",
        "nb_model.fit(X_train_text, y_train_text)\n",
        "\n",
        "# Évaluer\n",
        "accuracy_nb = nb_model.score(X_test_text, y_test_text)\n",
        "y_pred_nb = nb_model.predict(X_test_text)\n",
        "\n",
        "print(f\"\\nPerformance Naive Bayes sur texte:\")\n",
        "print(f\"  • Accuracy: {accuracy_nb:.4f} ({accuracy_nb:.1%})\")\n",
        "\n",
        "# Rapport détaillé\n",
        "print(f\"\\nRapport de classification détaillé:\")\n",
        "print(classification_report(y_test_text, y_pred_nb))\n",
        "\n",
        "# Analyser les mots les plus discriminants\n",
        "print(f\"\\nMots les plus discriminants par catégorie:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, categorie in enumerate(nb_model.classes_):\n",
        "    # Récupérer les log-probabilités des features pour cette classe\n",
        "    log_probs = nb_model.feature_log_prob_[i]\n",
        "\n",
        "    # Top 10 mots pour cette catégorie\n",
        "    top_indices = np.argsort(log_probs)[-10:]\n",
        "    top_words = [feature_names[idx] for idx in top_indices]\n",
        "    top_probs = [log_probs[idx] for idx in top_indices]\n",
        "\n",
        "    print(f\"\\n{categorie.upper()}:\")\n",
        "    for word, prob in zip(reversed(top_words), reversed(top_probs)):\n",
        "        print(f\"  • {word}: {prob:.3f}\")\n",
        "\n",
        "# Tester sur de nouveaux textes\n",
        "print(f\"\\nTest sur de nouveaux textes:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "nouveaux_textes = [\n",
        "    \"algorithme apprentissage automatique intelligence\",\n",
        "    \"match football équipe victoire\",\n",
        "    \"élection président vote démocratie\",\n",
        "    \"entreprise bourse investissement profit\"\n",
        "]\n",
        "\n",
        "nouveaux_textes_tfidf = vectorizer.transform(nouveaux_textes)\n",
        "predictions_nouveaux = nb_model.predict(nouveaux_textes_tfidf)\n",
        "probabilities_nouveaux = nb_model.predict_proba(nouveaux_textes_tfidf)\n",
        "\n",
        "for i, (texte, prediction, probs) in enumerate(zip(nouveaux_textes, predictions_nouveaux, probabilities_nouveaux)):\n",
        "    print(f\"\\nTexte {i+1}: '{texte}'\")\n",
        "    print(f\"  • Prédiction: {prediction}\")\n",
        "    print(f\"  • Confiance: {probs.max():.1%}\")\n",
        "    print(f\"  • Probabilités par classe:\")\n",
        "\n",
        "    for j, (classe, prob) in enumerate(zip(nb_model.classes_, probs)):\n",
        "        print(f\"    - {classe}: {prob:.1%}\")"
      ]
    }
  ]
}