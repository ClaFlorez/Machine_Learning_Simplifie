# Donn√©es pour le Machine Learning ‚Äî R√©sum√© simplifi√© (Module 2)
*Public : d√©butant ‚Äî Format : aide-m√©moire Markdown en fran√ßais*

---

## 2.1 Types de donn√©es : structur√©es vs non-structur√©es

### Objectifs
- Diff√©rencier **donn√©es structur√©es** et **non-structur√©es**  
- Voir **comment tout devient des nombres** pour un ordinateur  
- Parcourir **formats et exemples** (+ snippets Python)  
- Comprendre **avantages / d√©fis** et **choix par projet**

### La grande division
- **üìä Structur√©es** (tableaux, SQL) ‚Äî lignes/colonnes, format stable, faciles √† trier/analyser, pr√™tes pour le ML.  
- **üìÑ Non-structur√©es** (texte, image, audio, vid√©o) ‚Äî format libre, riches mais n√©cessitent preprocessing.  
> *Analogie* : les structur√©es = livre de recettes ; les non-structur√©es = histoires de grand‚Äëm√®re, passionnantes mais difficiles √† ranger.

### Donn√©es structur√©es ‚Äî exemple
**Base clients** : `ID, Nom, √Çge, Ville, Salaire, Abonn√©`

```python
# Lire/inspecter des donn√©es structur√©es
import pandas as pd

data = {
    'nom': ['Marie', 'Jean', 'Sophie', 'Pierre'],
    'age': [28, 34, 25, 42],
    'ville': ['Paris', 'Lyon', 'Marseille', 'Toulouse'],
    'salaire': [45000, 52000, 38000, 48000],
    'abonne': [True, False, True, True]
}
df = pd.DataFrame(data)
print(df)
print(df.describe())
print(df.dtypes)
```

**Types courants**  
- **Num√©riques** : entiers, d√©cimaux, % ‚Üí calculs directs.  
- **Cat√©gorielles** : nominales/ordinales/binaires ‚Üí *encodage requis*.  
- **Temporelles** : dates/heures/timestamps ‚Üí s√©ries temporelles.

### Donn√©es non-structur√©es ‚Äî exemples
- **Texte** (emails, articles, r√©seaux sociaux, avis, PDF)  
- **Multim√©dia** (images, vid√©os, audio, documents scann√©s)

```python
# Transformer des avis texte en tableau structur√© simple
import pandas as pd

avis_clients = [
    "Ce produit est fantastique ! Je le recommande vivement. Excellent service client.",
    "Tr√®s d√©√ßu de mon achat. La qualit√© n'est pas au rendez-vous. Service client lent.",
    "Produit correct, rien d'exceptionnel. Livraison rapide. Prix un peu √©lev√©.",
    "Absolument parfait ! D√©passe mes attentes. Bravo √† l'√©quipe !",
    "Mauvaise exp√©rience. Produit d√©fectueux. Remboursement difficile."
]

pos = ['fantastique','excellent','parfait','bravo','recommande','rapide']
neg = ['d√©√ßu','lent','mauvaise','d√©fectueux','difficile']

def sentiment(t):
    t=t.lower()
    sp=sum(w in t for w in pos); sn=sum(w in t for w in neg)
    return "Positif" if sp>sn else ("N√©gatif" if sn>sp else "Neutre")

df_avis = pd.DataFrame([{
    'id': i+1,
    'sentiment': sentiment(a),
    'longueur_mots': len(a.split()),
    'contient_service': 'service' in a.lower(),
    'contient_produit': 'produit' in a.lower(),
} for i,a in enumerate(avis_clients)])

print(df_avis)
```

### Comment l‚Äôordinateur ¬´ voit ¬ª les donn√©es (tout en nombres)
- **Texte ‚Üí vecteurs** : encodage (ASCII/Unicode), *tokens* ‚Üí indices, embeddings denses.  
- **Image ‚Üí tenseurs** : pixels **RGB** ‚àà [0,255] ‚Üí tableau `H√óW√ó3`.  

```python
# Image 3x3 en nombres
import numpy as np
image = np.array([
    [[255,0,0],[0,255,0],[0,0,255]],
    [[255,255,0],[255,0,255],[0,255,255]],
    [[0,0,0],[128,128,128],[255,255,255]]
], dtype=np.uint8)
print(image.shape, image.size, image.flatten()[:10])
```

### Avantages & d√©fis
| Aspect | Structur√©es | Non-structur√©es |
|---|---|---|
| Traitement | ‚úÖ Simple | ‚ùå Complexe |
| Richesse | ‚ö†Ô∏è Limit√©e | ‚úÖ Tr√®s riche |
| Pr√©paration | ‚úÖ Faible co√ªt | ‚ùå Pr√©processing lourd |
| Algorithmes | ‚úÖ Nombreux | ‚ö†Ô∏è Sp√©cialis√©s |
| Interpr√©tabilit√© | ‚úÖ Claire | ‚ùå Plus difficile |

### Semi‚Äëstructur√©es (entre‚Äëdeux)
- **JSON / XML / CSV + m√©tadonn√©es** : structure souple mais parsable.

### Choisir pour votre projet
- **Structur√©es si** tabulaires, d√©buts rapides, besoin d‚Äôexplicabilit√©.  
- **Non‚Äëstructur√©es si** beaucoup de texte/images, ressources de preprocessing, priorit√© performance.  

**Exercices** : classer des types, cr√©er un DataFrame √† partir d‚Äôun texte, compter mots +/- dans un avis, aplatir une image 2√ó2.

**√Ä retenir** : tout devient **nombres** ; 80% des donn√©es mondiales sont **non‚Äëstructur√©es**.

---

## 2.2 Qualit√© des donn√©es & principe GIGO

### GIGO (Garbage In, Garbage Out)
> **La qualit√© des pr√©dictions ‚â§ qualit√© des donn√©es d‚Äôentra√Ænement.**  
Mauvais ingr√©dients ‚áí mauvais plat, m√™me avec un grand chef.

### 6 dimensions cl√©s
**Exactitude ¬∑ Compl√©tude ¬∑ Coh√©rence ¬∑ Fra√Æcheur ¬∑ Pertinence ¬∑ Validit√©**

### Probl√®mes typiques ‚Üí impacts
- **Manquants / aberrants / doublons / formats incoh√©rents / obsol√®tes / typos** ‚Üí baisse de pr√©cision, surco√ªts, erreurs en prod.

```python
# Diagnostic rapide de qualit√©
import pandas as pd, numpy as np
df = pd.DataFrame({
    'nom':['Jean Dupont','marie martin','PIERRE DURAND',None,'Jean Dupont'],
    'age':[25,999,30,28,25],
    'email':['jean@gmail.com','marie@yahoo','pierre@outlook.com','invalide','jean@gmail.com'],
    'salaire':[45000,52000,None,48000,45000],
    'ville':['Paris','LYON','marseille','Paris','Paris'],
    'date_embauche':['2024-01-15','2023-13-45','2022-06-10','2024-02-30','2024-01-15']
})
print("NaN par colonne:
", df.isnull().sum())
print("Doublons:", df.duplicated().sum())
print("Emails invalides:", df[~df['email'].str.contains('@.*\.', na=False)]['email'].tolist())
```

```python
# Nettoyage minimal
df2 = df.drop_duplicates().copy()
df2['nom'] = df2['nom'].str.title()
df2['ville'] = df2['ville'].str.title()
median_age = df2['age'].median()
df2.loc[df2['age']>120,'age']=median_age
df2['salaire'] = df2['salaire'].fillna(df2['salaire'].mean())
print(df2)
```

**Strat√©gies manquants** : suppression (si <5%), imputation simple (moyenne/m√©diane/mode), ou **avanc√©e** (KNN/ML).

**Cas d‚Äô√©tude e‚Äëcommerce** : d√©doublonnage, normalisation, correction d‚Äô√¢ges, imputation (KNN) ‚Üí *dataset pr√™t ML*, KPI qualit√© ‚Üë.

**Impact qualit√©** : mauvaise qualit√© ‚áí pr√©cision ~65%, erreurs fr√©quentes, co√ªts √©lev√©s ; bonne qualit√© ‚áí pr√©cision ~90%+, confiance & ROI ‚Üë.

**Checklist qualit√©** : compl√©tude, exactitude, coh√©rence formats, unicit√©, validit√© m√©tier, fra√Æcheur, documentation.

---

## 2.3 Split Train / Validation / Test (r√®gle d‚Äôor)

### Pourquoi splitter ?
√âviter **l‚Äôoverfitting** : ne jamais √©valuer sur les donn√©es qui ont servi √† apprendre.

### Les 3 ensembles
- **Train (~70%)** : apprend les param√®tres.  
- **Validation (~15%)** : r√®gle les hyperparam√®tres, surveille overfitting.  
- **Test (~15%)** : *une seule fois √† la fin*, performance r√©elle.

```python
# Split 70/15/15 avec stratification
import pandas as pd, numpy as np
from sklearn.model_selection import train_test_split

np.random.seed(42)
n=1000
df = pd.DataFrame({
    'age': np.random.randint(18,70,n),
    'salaire': np.random.randint(25_000,80_000,n),
    'experience': np.random.randint(0,30,n),
    'formation': np.random.choice(['BAC','BAC+3','BAC+5'], n)
})
df['satisfaction'] = (df['salaire']>50_000).astype(int) + (df['experience']>5).astype(int)

X = df.drop('satisfaction', axis=1); y = df['satisfaction']

X_tmp, X_test, y_tmp, y_test = train_test_split(X,y,test_size=0.15,random_state=42,stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_tmp,y_tmp,test_size=0.176,random_state=42,stratify=y_tmp)
print(len(X_train), len(X_val), len(X_test))
```

**Proportions conseill√©es**  
<1k : 60/20/20 ¬∑ 1k‚Äì10k : 70/15/15 ¬∑ 10k‚Äì100k : 80/10/10 ¬∑ >100k : 85/7.5/7.5

**Validation crois√©e (K‚ÄëFold)** : pour *optimiser* et *stabiliser* l‚Äôestimation (surtout petits jeux).

**Bonnes pratiques** : split **avant** preprocessing, **stratifier** si classes d√©s√©quilibr√©es, *seed* fixe, pas de **data leakage**, test unique.

---

## 2.4 Biais & √©thique des donn√©es

### D√©finition
Distorsion syst√©matique (dans donn√©es/algorithmes) ‚Üí d√©cisions **injustes**.

### Types courants
- **S√©lection**, **historique**, **confirmation**, **√©tiquetage**, **algorithmique**, **culturel**.

### D√©tection (id√©es)
- Comparer taux/pr√©cision par **sous‚Äëgroupes**, tests statistiques (œá¬≤), importance de variables sensibles, audits.

```python
# Sch√©ma minimal d‚Äôanalyse par sous-groupes (ex. genre)
# (√† adapter √† vos donn√©es r√©elles)
# 1) Taux positifs par groupe, 2) tests œá¬≤, 3) √©carts TPR/PPR.
```

### R√©duction des biais
- **Pr√©ventif** : √©chantillons √©quilibr√©s, collecte diversifi√©e, labels objectifs, audit amont.  
- **Correctif** : r√©√©chantillonnage, retrait de variables sensibles, contraintes d‚Äô√©quit√©, post‚Äëprocessing.

**M√©triques d‚Äô√©quit√©** : parit√© d√©mographique, √©galit√© des chances, parit√© calibr√©e (*souvent impossibles √† satisfaire simultan√©ment*).

**Principes d‚ÄôIA responsable** : transparence, √©quit√©, vie priv√©e, responsabilit√©, robustesse, b√©n√©fice humain, auditabilit√©.  
**Gouvernance** : documentation, tests d‚Äô√©quit√©, monitoring continu, recours utilisateur, mise √† jour r√©guli√®re.  
**R√©glementation (ex.)** : RGPD/AI Act (UE), r√®gles sectorielles (US), cadres Canada, etc.

---

## 2.5 Sources & acquisition de donn√©es

### O√π trouver des donn√©es ?
- **Gratuit** : **Kaggle**, **UCI**, **Google Dataset Search** (id√©als pour apprendre).  
- **Sp√©cialis√©** : gouvernement (Data.gouv.fr, Data.gov), finance (Yahoo/Quandl/Alpha Vantage), sant√© (MIMIC/PhysioNet), g√©o (OSM/NASA), r√©seaux sociaux (APIs), images (ImageNet/COCO), texte (Common Crawl).

**Datasets ¬´ Hello World ¬ª** : Iris, Titanic, Boston Housing, Wine.  
**Interm√©diaires** : MNIST, CIFAR‚Äë10, MovieLens, IMDb Reviews.

```python
# Chargements rapides (extraits)
from sklearn import datasets
import pandas as pd

iris = datasets.load_iris()
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names).assign(species=iris.target)
print(df_iris.head())

# Sauvegarde rapide
df_iris.to_csv('iris.csv', index=False)
```

### APIs temps r√©el & scraping √©thique
- Pr√©f√©rer **APIs officielles** (quotas, ToS).  
- **Scraping responsable** : robots.txt, d√©lais, User‚ÄëAgent, pas de donn√©es perso, droits d‚Äôauteur.

### Cr√©er ses propres donn√©es
- Enqu√™tes (Forms/Typeform), logs produits, capteurs, **donn√©es synth√©tiques** pour prototypage.

```python
# G√©n√©rer jeu de classification synth√©tique
from sklearn.datasets import make_classification
import pandas as pd
X,y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=3, random_state=42)
pd.DataFrame(X).assign(target=y).to_csv('synthetic_classification.csv', index=False)
```

### L√©gal & RGPD (essentiel)
- **OK** : donn√©es publiques, anonymis√©es, consentement explicite, APIs conformes.  
- **√Ä √©viter** : donn√©es perso sans base l√©gale, scraping interdit, donn√©es sensibles, contenus prot√©g√©s.

**Checklist conformit√©** : l√©galit√© source, consentements, anonymisation, s√©curit√©/acc√®s, dur√©e de conservation, droits des personnes, documentation.

---

## Points cl√©s (Module 2)
- **Types** : structur√©es (faciles) vs non‚Äëstructur√©es (riches, √† pr√©parer).  
- **Qualit√©** : GIGO ‚Äî investir massivement dans nettoyage/validation.  
- **Split** : Train/Val/Test & CV pour √©valuer **g√©n√©ralisation**.  
- **√âquit√©** : d√©tecter/corriger les biais ; principes d‚ÄôIA responsable.  
- **Sources** : plateformes ouvertes, APIs, donn√©es synth√©tiques ; **RGPD** √† respecter.

*Fin du Module 2 ‚Äî pr√™t pour la pratique Python dans le Module 3.*
