# Module 4 ‚Ä¢ Chapitre 1  
## 4.1 ‚Äì Choisir le bon algorithme  
**Niveau : D√©butant ‚Ä¢ Dur√©e : ~30-35 min**  

---

## üéØ Objectifs d‚Äôapprentissage
- Comprendre les types de probl√®mes de Machine Learning et leurs algorithmes.  
- Identifier le type de probl√®me (supervis√©, non supervis√©, renforcement).  
- Suivre un guide de s√©lection d‚Äôalgorithmes.  
- Conna√Ætre les avantages et limites de chaque approche.  
- R√©aliser vos premiers tests avec Scikit-learn.  
- D√©finir une strat√©gie pour bien d√©buter.  

---

## üöÄ Les 3 grandes familles de Machine Learning

### Supervis√©  
- Donn√©es √©tiquet√©es (X, y).  
- Objectif : pr√©dire une cible connue.  
- Cas : classification ou r√©gression.  
- 90% des probl√®mes ML.  

### Non supervis√©  
- Donn√©es non √©tiquet√©es (X seulement).  
- Objectif : d√©couvrir des structures cach√©es.  
- Cas : clustering, r√©duction de dimension, d√©tection d‚Äôanomalies.  

### Par renforcement  
- Agent qui apprend par essai-erreur.  
- Interaction avec un environnement, r√©compenses.  
- Cas : jeux, robotique, trading.  
- Plus avanc√©, tr√®s sp√©cialis√©.  

---

## üîç Identifier votre type de probl√®me
1. **Avez-vous des donn√©es avec la ‚Äúbonne r√©ponse‚Äù ?**  
   - ‚úÖ Oui ‚Üí **Supervis√©**  
   - ‚ùå Non ‚Üí **Non supervis√©**  

2. **Si supervis√© :**  
   - Cat√©gorie ‚Üí Classification.  
   - Nombre ‚Üí R√©gression.  

3. **Si non supervis√© :**  
   - Groupes similaires ‚Üí Clustering.  
   - Simplification ‚Üí R√©duction de dimension.  
   - D√©tection d‚Äôanormal ‚Üí Anomalies.  

---

## üå≥ Algorithmes principaux

### Classification  
- **Decision Tree** : simple, intuitif mais risque d‚Äôoverfitting.  
- **Logistic Regression** : rapide, robuste, baseline solide.  
- **Random Forest** : performant, robuste, recommand√©.  
- **KNN** : tr√®s simple, utile sur petits datasets.  

### R√©gression  
- **Linear Regression** : rapide, interpr√©table, baseline id√©ale.  
- **Decision Tree Regressor** : g√®re non-lin√©arit√©s mais risque d‚Äôoverfitting.  
- **Random Forest Regressor** : robuste, performant, choix s√ªr.  
- **Support Vector Regression** : flexible mais complexe et lent.  

---

## üß≠ Guide pratique de s√©lection
1. **Commencez simple :**  
   - Classification ‚Üí Logistic Regression, Random Forest.  
   - R√©gression ‚Üí Linear Regression, Random Forest.  

2. **Testez rapidement plusieurs mod√®les.**  

3. **Optimisez ensuite :** tuning, features, mod√®les avanc√©s.  

4. **Validez rigoureusement :** cross-validation, m√©triques multiples.  

---

## üìä Arbre de d√©cision pratique
- Peu de donn√©es (< 1000) ‚Üí KNN, Logistic Regression, Linear Regression.  
- Beaucoup de donn√©es (> 10k) ‚Üí Random Forest, SVM, Neural Networks.  
- Besoin de vitesse ‚Üí Linear/Logistic Regression, Naive Bayes.  
- Besoin d‚Äôinterpr√©tation ‚Üí Decision Tree, Linear/Logistic Regression.  
- Performance maximale ‚Üí Random Forest, XGBoost, ensembles.  

---

## üß™ Premier test pratique avec Scikit-learn

```python
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, mean_squared_error

print("üß™ TEST D'ALGORITHMES ML")
print("=" * 25)

# 1. Classification
X_class, y_class = make_classification(n_samples=1000, n_features=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.2, random_state=42)

classifiers = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'KNN': KNeighborsClassifier()
}

for name, model in classifiers.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name}: {accuracy:.3f}")

# 2. R√©gression
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

regressors = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'KNN': KNeighborsRegressor()
}

for name, model in regressors.items():
    model.fit(X_train_reg, y_train_reg)
    y_pred_reg = model.predict(X_test_reg)
    mse = mean_squared_error(y_test_reg, y_pred_reg)
    print(f"{name}: MSE={mse:.2f}")
```

---

## üìå Cas d‚Äôusage par secteur
| Secteur      | Probl√®me typique            | Type ML         | Algo recommand√©               |
|--------------|-----------------------------|----------------|-------------------------------|
| E-commerce   | Recommandations produits    | Supervis√©       | Random Forest, Collaborative Filtering |
| Finance      | D√©tection fraude            | Classification | Random Forest, XGBoost        |
| Immobilier   | Estimation prix             | R√©gression     | Random Forest, Linear Regression |
| Marketing    | Segmentation clients        | Non supervis√©  | K-Means, Clustering hi√©rarchique |
| Sant√©        | Diagnostic m√©dical          | Classification | SVM, Random Forest            |
| Transport    | Optimisation routes         | R√©gression     | Linear Regression, Neural Nets |

---

## ‚ö†Ô∏è Erreurs fr√©quentes √† √©viter
- Commencer trop complexe (neural networks).  
- Ignorer les baselines simples.  
- Optimiser trop t√¥t.  
- Se limiter √† une seule m√©trique.  
- Tester uniquement sur les donn√©es d‚Äôentra√Ænement.  
- Ne pas pr√©parer correctement les donn√©es.  

‚úÖ **Bonnes pratiques :**  
Commencer simple, tester plusieurs mod√®les, valider avec cross-validation, utiliser plusieurs m√©triques, bien comprendre les donn√©es, it√©rer rapidement.  

---

## üó∫Ô∏è Strat√©gie recommand√©e
1. D√©finir le probl√®me.  
2. Construire une baseline simple.  
3. Utiliser un Random Forest comme r√©f√©rence.  
4. Comparer plusieurs algorithmes.  
5. Optimiser le meilleur mod√®le.  

---

## ‚ú® Points cl√©s √† retenir
- Commencez simple : Linear/Logistic Regression ‚Üí Random Forest.  
- Testez plusieurs algorithmes avant d‚Äôoptimiser.  
- Pas de solution magique : tout d√©pend des donn√©es.  
- Validation crois√©e essentielle pour comparer.  
- It√©rez rapidement : prototype ‚Üí test ‚Üí am√©lioration.  
- Random Forest : excellent compromis pour d√©buter.  
