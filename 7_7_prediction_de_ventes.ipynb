{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuxnR+jjyjhZLQMYVM8NMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaFlorez/Machine_Learning_Simplifie/blob/main/7_7_prediction_de_ventes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-tevlJVUO0d",
        "outputId": "9b9a213a-8ff1-411b-a208-fd7d3d7222fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJET: Système de prédiction de ventes pour chaîne de magasins\n",
            "================================================================================\n",
            "Dataset de ventes créé:\n",
            "  Période: 2022-01-01 à 2023-12-23\n",
            "  Nombre de magasins: 50\n",
            "  Nombre de semaines: 52\n",
            "  Total d'observations: 5,200\n",
            "  Ventes moyennes: 87,441€/semaine\n",
            "  Écart-type: 35,419€\n",
            "\n",
            "Exploration des données:\n",
            "========================================\n",
            "Ventes moyennes par caractéristiques:\n",
            "\n",
            "ville_taille:\n",
            "  Grande: 111,506€\n",
            "  Moyenne: 94,515€\n",
            "  Petite: 73,407€\n",
            "\n",
            "zone_type:\n",
            "  Centre: 102,643€\n",
            "  Commercial: 91,979€\n",
            "  Banlieue: 77,921€\n",
            "\n",
            "Corrélations avec les ventes:\n",
            "  surface: +0.634\n",
            "  nb_employes: +0.347\n",
            "  pouvoir_achat_local: +0.142\n",
            "  concurrence: -0.124\n",
            "\n",
            "Features préparées:\n",
            "  Nombre de features: 14\n",
            "  Features: ['week_of_year', 'month', 'is_holiday', 'is_winter', 'surface', 'nb_employes', 'pouvoir_achat_local', 'concurrence', 'ville_Grande', 'ville_Moyenne', 'ville_Petite', 'zone_Banlieue', 'zone_Centre', 'zone_Commercial']\n",
            "\n",
            "Division temporelle des données:\n",
            "  Entraînement: 4,160 observations\n",
            "  Test: 1,040 observations\n",
            "  Période test: dernières 20 semaines\n",
            "\n",
            "Comparaison des modèles de régression:\n",
            "================================================================================\n",
            "Modèle               R² Train     R² Test      RMSE Test       MAE Test\n",
            "--------------------------------------------------------------------------------\n",
            "Linéaire Simple      0.7846       0.7432       15,821          12,331€\n",
            "Ridge (α=1.0)        0.7846       0.7432       15,821          12,331€\n",
            "Lasso (α=100)        0.7845       0.7427       15,836          12,362€\n",
            "Polynomiale Deg2     0.8869       0.8596       11,700          9,088€\n",
            "\n",
            "Meilleur modèle: Polynomiale Deg2\n",
            "  R² test: 0.8596\n",
            "  RMSE test: 11,700€\n",
            "  MAE test: 9,088€\n",
            "\n",
            "Analyse des prédictions par magasin:\n",
            "==================================================\n",
            "Top 5 magasins avec meilleures prédictions:\n",
            "  Magasin 41: erreur moyenne 10.0%\n",
            "  Magasin 45: erreur moyenne 10.0%\n",
            "  Magasin 42: erreur moyenne 11.0%\n",
            "  Magasin 43: erreur moyenne 11.0%\n",
            "  Magasin 44: erreur moyenne 11.0%\n",
            "\n",
            "Top 5 magasins avec prédictions les plus difficiles:\n",
            "  Magasin 48: erreur moyenne 12.0%\n",
            "  Magasin 49: erreur moyenne 12.0%\n",
            "  Magasin 42: erreur moyenne 11.0%\n",
            "  Magasin 43: erreur moyenne 11.0%\n",
            "  Magasin 44: erreur moyenne 11.0%\n",
            "\n",
            "Analyse temporelle des prédictions:\n",
            "==================================================\n",
            "Performance par mois:\n",
            "  Jan: 11.0% d'erreur moyenne\n",
            "  Fév: 9.7% d'erreur moyenne\n",
            "  Mar: 10.3% d'erreur moyenne\n",
            "  Avr: 8.9% d'erreur moyenne\n",
            "  Mai: 8.6% d'erreur moyenne\n",
            "  Jun: 10.2% d'erreur moyenne\n",
            "  Jul: 9.3% d'erreur moyenne\n",
            "  Aoû: 9.6% d'erreur moyenne\n",
            "  Sep: 16.1% d'erreur moyenne\n",
            "  Oct: 11.8% d'erreur moyenne\n",
            "  Nov: 14.7% d'erreur moyenne\n",
            "  Déc: 10.8% d'erreur moyenne\n",
            "\n",
            "Impact business des erreurs de prédiction:\n",
            "============================================================\n",
            "Coûts estimés sur la période de test:\n",
            "  Surstockage: 102,612€\n",
            "  Ruptures: 648,188€\n",
            "  TOTAL: 750,800€\n",
            "\n",
            "Comparaison avec prédiction naïve (moyenne):\n",
            "  Coût avec moyenne historique: 2,140,809€\n",
            "  Coût avec notre modèle: 750,800€\n",
            "  ÉCONOMIE RÉALISÉE: 1,390,009€\n",
            "  Amélioration: 64.9%\n",
            "\n",
            "Recommandations opérationnelles:\n",
            "==================================================\n",
            "✅ EXCELLENT modèle - Déploiement recommandé\n",
            "  • Utiliser pour les commandes automatiques\n",
            "  • Monitorer les performances hebdomadairement\n",
            "\n",
            "Stratégies d'amélioration identifiées:\n",
            "==================================================\n",
            "Magasins difficiles à prédire:\n",
            "  Magasin 48: Moyenne ville, Banlieue\n",
            "  Magasin 49: Petite ville, Banlieue\n",
            "  Magasin 42: Petite ville, Banlieue\n",
            "  Magasin 43: Petite ville, Commercial\n",
            "  Magasin 44: Moyenne ville, Centre\n",
            "\n",
            "Actions recommandées:\n",
            "  • Analyser spécifiquement ces magasins\n",
            "  • Collecter des données supplémentaires\n",
            "  • Créer des modèles spécialisés si nécessaire\n",
            "\n",
            "Périodes difficiles à prédire:\n",
            "  Sep: 16.1% d'erreur\n",
            "  Nov: 14.7% d'erreur\n",
            "\n",
            "Actions recommandées:\n",
            "  • Ajouter des features saisonnières spécifiques\n",
            "  • Intégrer les données météorologiques\n",
            "  • Considérer les événements locaux\n",
            "\n",
            "Système d'alertes proposé:\n",
            "========================================\n",
            "ALERTE ROUGE (erreur >20%):\n",
            "  • Révision manuelle obligatoire\n",
            "  • Investigation des causes\n",
            "\n",
            "ALERTE ORANGE (erreur 10-20%):\n",
            "  • Validation par le manager\n",
            "  • Ajustement possible\n",
            "\n",
            "VERT (erreur <10%):\n",
            "  • Commande automatique\n",
            "  • Monitoring standard\n",
            "\n",
            "Métriques de succès pour le suivi en production:\n",
            "============================================================\n",
            "KPIs à surveiller:\n",
            "  • RMSE < 12,870€ (tolérance +10%)\n",
            "  • MAE < 9,997€\n",
            "  • R² > 0.774 (tolérance -10%)\n",
            "  • Erreur relative moyenne < 15%\n",
            "\n",
            "Fréquence de réentraînement:\n",
            "  • Réentraînement mensuel recommandé\n",
            "  • Réentraînement d'urgence si performance dégradée\n",
            "  • Intégration de nouvelles features trimestrielle\n",
            "\n",
            "ROI estimé du projet:\n",
            "==============================\n",
            "  Économie annuelle estimée: 3,614,024€\n",
            "  Coût de développement: 50,000€\n",
            "  ROI première année: 7128%\n",
            "  ✅ Projet très rentable!\n"
          ]
        }
      ],
      "source": [
        "#Projet complet de prédiction de ventes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.pipeline import Pipeline # Import Pipeline\n",
        "\n",
        "# Créer un dataset de ventes réaliste\n",
        "print(\"PROJET: Système de prédiction de ventes pour chaîne de magasins\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "np.random.seed(42)\n",
        "n_weeks = 104  # 2 ans de données hebdomadaires\n",
        "n_stores = 50  # 50 magasins\n",
        "\n",
        "# Générer les caractéristiques des magasins\n",
        "store_data = []\n",
        "for store_id in range(1, n_stores + 1):\n",
        "    # Caractéristiques fixes du magasin\n",
        "    surface = np.random.uniform(200, 1000)  # m²\n",
        "    nb_employes = np.random.poisson(8) + 3  # 3-15 employés\n",
        "    ville_taille = np.random.choice(['Petite', 'Moyenne', 'Grande'], p=[0.4, 0.4, 0.2])\n",
        "    zone_type = np.random.choice(['Centre', 'Banlieue', 'Commercial'], p=[0.3, 0.5, 0.2])\n",
        "\n",
        "    # Facteurs économiques locaux\n",
        "    pouvoir_achat_local = np.random.normal(1.0, 0.2)  # Multiplicateur\n",
        "    concurrence = np.random.poisson(3) + 1  # Nombre de concurrents\n",
        "\n",
        "    store_data.append({\n",
        "        'store_id': store_id,\n",
        "        'surface': surface,\n",
        "        'nb_employes': nb_employes,\n",
        "        'ville_taille': ville_taille,\n",
        "        'zone_type': zone_type,\n",
        "        'pouvoir_achat_local': pouvoir_achat_local,\n",
        "        'concurrence': concurrence\n",
        "    })\n",
        "\n",
        "stores_df = pd.DataFrame(store_data)\n",
        "\n",
        "# Générer les données de ventes hebdomadaires\n",
        "sales_data = []\n",
        "base_date = datetime(2022, 1, 1)\n",
        "\n",
        "for week in range(n_weeks):\n",
        "    current_date = base_date + timedelta(weeks=week)\n",
        "\n",
        "    # Facteurs temporels\n",
        "    month = current_date.month\n",
        "    is_holiday = month in [7, 8, 12]  # Vacances d'été et Noël\n",
        "    is_winter = month in [12, 1, 2]\n",
        "    week_of_year = current_date.isocalendar()[1]\n",
        "\n",
        "    # Tendance générale (croissance de l'entreprise)\n",
        "    trend_factor = 1 + (week * 0.002)  # 0.2% de croissance par semaine\n",
        "\n",
        "    # Saisonnalité\n",
        "    seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * week / 52)  # Cycle annuel\n",
        "\n",
        "    for _, store in stores_df.iterrows():\n",
        "        # Ventes de base selon les caractéristiques du magasin\n",
        "        ventes_base = (\n",
        "            store['surface'] * 50 +  # 50€ de CA par m²\n",
        "            store['nb_employes'] * 2000 +  # 2000€ par employé\n",
        "            store['pouvoir_achat_local'] * 10000 -  # Impact pouvoir d'achat\n",
        "            store['concurrence'] * 1000  # Impact concurrence\n",
        "        )\n",
        "\n",
        "        # Ajustements selon la localisation\n",
        "        if store['ville_taille'] == 'Grande':\n",
        "            ventes_base *= 1.3\n",
        "        elif store['ville_taille'] == 'Moyenne':\n",
        "            ventes_base *= 1.1\n",
        "\n",
        "        if store['zone_type'] == 'Centre':\n",
        "            ventes_base *= 1.2\n",
        "        elif store['zone_type'] == 'Commercial':\n",
        "            ventes_base *= 1.4\n",
        "\n",
        "        # Appliquer les facteurs temporels\n",
        "        ventes_semaine = ventes_base * trend_factor * seasonal_factor\n",
        "\n",
        "        # Effets spéciaux\n",
        "        if is_holiday:\n",
        "            ventes_semaine *= 1.5  # Boost pendant les vacances\n",
        "\n",
        "        if is_winter and store['zone_type'] == 'Commercial':\n",
        "            ventes_semaine *= 0.8  # Baisse hivernale pour centres commerciaux\n",
        "\n",
        "        # Ajouter du bruit réaliste\n",
        "        bruit = np.random.normal(0, ventes_semaine * 0.1)  # 10% de variabilité\n",
        "        ventes_finale = max(0, ventes_semaine + bruit)  # Pas de ventes négatives\n",
        "\n",
        "        sales_data.append({\n",
        "            'store_id': store['store_id'],\n",
        "            'date': current_date,\n",
        "            'week_of_year': week_of_year,\n",
        "            'month': month,\n",
        "            'is_holiday': is_holiday,\n",
        "            'is_winter': is_winter,\n",
        "            'ventes': ventes_finale,\n",
        "            'surface': store['surface'],\n",
        "            'nb_employes': store['nb_employes'],\n",
        "            'ville_taille': store['ville_taille'],\n",
        "            'zone_type': store['zone_type'],\n",
        "            'pouvoir_achat_local': store['pouvoir_achat_local'],\n",
        "            'concurrence': store['concurrence']\n",
        "        })\n",
        "\n",
        "# Créer le DataFrame final\n",
        "df = pd.DataFrame(sales_data)\n",
        "\n",
        "print(f\"Dataset de ventes créé:\")\n",
        "print(f\"  Période: {df['date'].min().strftime('%Y-%m-%d')} à {df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"  Nombre de magasins: {df['store_id'].nunique()}\")\n",
        "print(f\"  Nombre de semaines: {df['week_of_year'].nunique()}\")\n",
        "print(f\"  Total d'observations: {len(df):,}\")\n",
        "print(f\"  Ventes moyennes: {df['ventes'].mean():,.0f}€/semaine\")\n",
        "print(f\"  Écart-type: {df['ventes'].std():,.0f}€\")\n",
        "\n",
        "# Exploration des données\n",
        "print(f\"\\nExploration des données:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Ventes par type de magasin\n",
        "print(f\"Ventes moyennes par caractéristiques:\")\n",
        "for col in ['ville_taille', 'zone_type']:\n",
        "    print(f\"\\n{col}:\")\n",
        "    avg_by_cat = df.groupby(col)['ventes'].mean().sort_values(ascending=False)\n",
        "    for cat, avg in avg_by_cat.items():\n",
        "        print(f\"  {cat}: {avg:,.0f}€\")\n",
        "\n",
        "# Corrélations avec les ventes\n",
        "numeric_cols = ['ventes', 'surface', 'nb_employes', 'pouvoir_achat_local', 'concurrence']\n",
        "correlations = df[numeric_cols].corr()['ventes'].sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nCorrélations avec les ventes:\")\n",
        "for col, corr in correlations.items():\n",
        "    if col != 'ventes':\n",
        "        print(f\"  {col}: {corr:+.3f}\")\n",
        "\n",
        "# Préparer les features pour la régression\n",
        "# Encoder les variables catégorielles\n",
        "df_encoded = pd.get_dummies(df, columns=['ville_taille', 'zone_type'], prefix=['ville', 'zone'])\n",
        "\n",
        "# Features pour la prédiction\n",
        "feature_cols = [col for col in df_encoded.columns if col not in ['store_id', 'date', 'ventes']]\n",
        "X = df_encoded[feature_cols]\n",
        "y = df_encoded['ventes']\n",
        "\n",
        "print(f\"\\nFeatures préparées:\")\n",
        "print(f\"  Nombre de features: {len(feature_cols)}\")\n",
        "print(f\"  Features: {feature_cols}\")\n",
        "\n",
        "# Diviser les données (attention: données temporelles!)\n",
        "# On ne peut pas mélanger - les dernières semaines servent de test\n",
        "split_point = int(len(df) * 0.8)  # 80% pour train, 20% pour test\n",
        "df_sorted = df_encoded.sort_values(['store_id', 'date'])\n",
        "\n",
        "X_train = df_sorted[feature_cols].iloc[:split_point]\n",
        "X_test = df_sorted[feature_cols].iloc[split_point:]\n",
        "y_train = df_sorted['ventes'].iloc[:split_point]\n",
        "y_test = df_sorted['ventes'].iloc[split_point:]\n",
        "\n",
        "print(f\"\\nDivision temporelle des données:\")\n",
        "print(f\"  Entraînement: {len(X_train):,} observations\")\n",
        "print(f\"  Test: {len(X_test):,} observations\")\n",
        "print(f\"  Période test: dernières {len(X_test) // n_stores} semaines\")\n",
        "\n",
        "# Standardiser les features numériques\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Comparer plusieurs approches de régression\n",
        "models = {\n",
        "    'Linéaire Simple': LinearRegression(),\n",
        "    'Ridge (α=1.0)': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso (α=100)': Lasso(alpha=100, random_state=42, max_iter=2000),\n",
        "    'Polynomiale Deg2': Pipeline([\n",
        "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('ridge', Ridge(alpha=10, random_state=42))\n",
        "    ])\n",
        "}\n",
        "\n",
        "print(f\"\\nComparaison des modèles de régression:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Modèle':<20} {'R² Train':<12} {'R² Test':<12} {'RMSE Test':<15} {'MAE Test'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if 'Polynomiale' in name:\n",
        "        # Pour le modèle polynomial, utiliser les données non standardisées\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "    else:\n",
        "        # Pour les autres, utiliser les données standardisées\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred_train = model.predict(X_train_scaled)\n",
        "        y_pred_test = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculer les métriques\n",
        "    r2_train = r2_score(y_train, y_pred_train)\n",
        "    r2_test = r2_score(y_test, y_pred_test)\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "    model_results[name] = {\n",
        "        'r2_train': r2_train,\n",
        "        'r2_test': r2_test,\n",
        "        'rmse_test': rmse_test,\n",
        "        'mae_test': mae_test,\n",
        "        'predictions': y_pred_test\n",
        "    }\n",
        "\n",
        "    print(f\"{name:<20} {r2_train:<12.4f} {r2_test:<12.4f} {rmse_test:<15,.0f} {mae_test:,.0f}€\")\n",
        "\n",
        "# Identifier le meilleur modèle\n",
        "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['r2_test'])\n",
        "best_result = model_results[best_model_name]\n",
        "\n",
        "print(f\"\\nMeilleur modèle: {best_model_name}\")\n",
        "print(f\"  R² test: {best_result['r2_test']:.4f}\")\n",
        "print(f\"  RMSE test: {best_result['rmse_test']:,.0f}€\")\n",
        "print(f\"  MAE test: {best_result['mae_test']:,.0f}€\")\n",
        "\n",
        "# Analyser les prédictions par magasin\n",
        "print(f\"\\nAnalyse des prédictions par magasin:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Ajouter les prédictions au DataFrame de test\n",
        "df_test = df_sorted.iloc[split_point:].copy()\n",
        "df_test['predictions'] = best_result['predictions']\n",
        "df_test['erreur_absolue'] = np.abs(df_test['ventes'] - df_test['predictions'])\n",
        "df_test['erreur_relative'] = df_test['erreur_absolue'] / df_test['ventes'] * 100\n",
        "\n",
        "# Performance par magasin\n",
        "perf_by_store = df_test.groupby('store_id').agg({\n",
        "    'ventes': 'mean',\n",
        "    'predictions': 'mean',\n",
        "    'erreur_absolue': 'mean',\n",
        "    'erreur_relative': 'mean'\n",
        "}).round(0)\n",
        "\n",
        "# Top 5 meilleurs et pires magasins\n",
        "best_stores = perf_by_store.nsmallest(5, 'erreur_relative')\n",
        "worst_stores = perf_by_store.nlargest(5, 'erreur_relative')\n",
        "\n",
        "print(f\"Top 5 magasins avec meilleures prédictions:\")\n",
        "for store_id, row in best_stores.iterrows():\n",
        "    print(f\"  Magasin {store_id}: erreur moyenne {row['erreur_relative']:.1f}%\")\n",
        "\n",
        "print(f\"\\nTop 5 magasins avec prédictions les plus difficiles:\")\n",
        "for store_id, row in worst_stores.iterrows():\n",
        "    print(f\"  Magasin {store_id}: erreur moyenne {row['erreur_relative']:.1f}%\")\n",
        "\n",
        "# Analyse temporelle\n",
        "print(f\"\\nAnalyse temporelle des prédictions:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Performance par mois\n",
        "df_test['month'] = df_test['date'].dt.month\n",
        "perf_by_month = df_test.groupby('month').agg({\n",
        "    'erreur_relative': 'mean',\n",
        "    'ventes': 'mean'\n",
        "}).round(1)\n",
        "\n",
        "print(f\"Performance par mois:\")\n",
        "mois_noms = ['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Jun',\n",
        "             'Jul', 'Aoû', 'Sep', 'Oct', 'Nov', 'Déc']\n",
        "\n",
        "for month, row in perf_by_month.iterrows():\n",
        "    nom_mois = mois_noms[month-1] if 1 <= month <= 12 else f\"Mois {month}\"\n",
        "    print(f\"  {nom_mois}: {row['erreur_relative']:.1f}% d'erreur moyenne\")\n",
        "\n",
        "# Impact business des erreurs\n",
        "print(f\"\\nImpact business des erreurs de prédiction:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simuler les coûts\n",
        "cout_surstockage = 0.02  # 2% du CA en coût de stockage par semaine\n",
        "cout_rupture = 0.15  # 15% de perte de CA par rupture\n",
        "\n",
        "surestimations = df_test['predictions'] > df_test['ventes']\n",
        "sous_estimations = df_test['predictions'] < df_test['ventes']\n",
        "\n",
        "# Calculs des coûts\n",
        "erreurs_surestimation = df_test[surestimations]['predictions'] - df_test[surestimations]['ventes']\n",
        "erreurs_sous_estimation = df_test[sous_estimations]['ventes'] - df_test[sous_estimations]['predictions']\n",
        "\n",
        "cout_total_surstock = (erreurs_surestimation * cout_surstockage).sum()\n",
        "cout_total_rupture = (erreurs_sous_estimation * cout_rupture).sum()\n",
        "cout_total = cout_total_surstock + cout_total_rupture\n",
        "\n",
        "print(f\"Coûts estimés sur la période de test:\")\n",
        "print(f\"  Surstockage: {cout_total_surstock:,.0f}€\")\n",
        "print(f\"  Ruptures: {cout_total_rupture:,.0f}€\")\n",
        "print(f\"  TOTAL: {cout_total:,.0f}€\")\n",
        "\n",
        "# Comparer avec une prédiction naïve (moyenne historique)\n",
        "ventes_moyenne_historique = y_train.mean()\n",
        "erreur_naive = np.abs(y_test - ventes_moyenne_historique)\n",
        "cout_naive = (erreur_naive * (cout_surstockage + cout_rupture) / 2).sum()\n",
        "\n",
        "economie = cout_naive - cout_total\n",
        "print(f\"\\nComparaison avec prédiction naïve (moyenne):\")\n",
        "print(f\"  Coût avec moyenne historique: {cout_naive:,.0f}€\")\n",
        "print(f\"  Coût avec notre modèle: {cout_total:,.0f}€\")\n",
        "print(f\"  ÉCONOMIE RÉALISÉE: {economie:,.0f}€\")\n",
        "print(f\"  Amélioration: {economie/cout_naive*100:.1f}%\")\n",
        "\n",
        "# Recommandations opérationnelles\n",
        "print(f\"\\nRecommandations opérationnelles:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if best_result['r2_test'] > 0.8:\n",
        "    print(\"✅ EXCELLENT modèle - Déploiement recommandé\")\n",
        "    print(\"  • Utiliser pour les commandes automatiques\")\n",
        "    print(\"  • Monitorer les performances hebdomadairement\")\n",
        "elif best_result['r2_test'] > 0.6:\n",
        "    print(\"✅ BON modèle - Déploiement avec supervision\")\n",
        "    print(\"  • Utiliser comme aide à la décision\")\n",
        "    print(\"  • Valider manuellement les commandes importantes\")\n",
        "else:\n",
        "    print(\"⚠️ MODÈLE PERFECTIBLE - Amélioration nécessaire\")\n",
        "    print(\"  • Collecter plus de données\")\n",
        "    print(\"  • Ajouter des features (météo, événements locaux)\")\n",
        "\n",
        "# Stratégies d'amélioration\n",
        "print(f\"\\nStratégies d'amélioration identifiées:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyser les magasins difficiles à prédire\n",
        "if len(worst_stores) > 0:\n",
        "    print(f\"Magasins difficiles à prédire:\")\n",
        "    for store_id, _ in worst_stores.iterrows():\n",
        "        store_info = stores_df[stores_df['store_id'] == store_id].iloc[0]\n",
        "        print(f\"  Magasin {store_id}: {store_info['ville_taille']} ville, {store_info['zone_type']}\")\n",
        "\n",
        "    print(f\"\\nActions recommandées:\")\n",
        "    print(\"  • Analyser spécifiquement ces magasins\")\n",
        "    print(\"  • Collecter des données supplémentaires\")\n",
        "    print(\"  • Créer des modèles spécialisés si nécessaire\")\n",
        "\n",
        "# Analyser les périodes difficiles\n",
        "mois_difficiles = perf_by_month[perf_by_month['erreur_relative'] > perf_by_month['erreur_relative'].mean() + perf_by_month['erreur_relative'].std()]\n",
        "\n",
        "if len(mois_difficiles) > 0:\n",
        "    print(f\"\\nPériodes difficiles à prédire:\")\n",
        "    for month, row in mois_difficiles.iterrows():\n",
        "        nom_mois = mois_noms[month-1]\n",
        "        print(f\"  {nom_mois}: {row['erreur_relative']:.1f}% d'erreur\")\n",
        "\n",
        "    print(f\"\\nActions recommandées:\")\n",
        "    print(\"  • Ajouter des features saisonnières spécifiques\")\n",
        "    print(\"  • Intégrer les données météorologiques\")\n",
        "    print(\"  • Considérer les événements locaux\")\n",
        "\n",
        "# Système d'alertes\n",
        "print(f\"\\nSystème d'alertes proposé:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"ALERTE ROUGE (erreur >20%):\")\n",
        "print(\"  • Révision manuelle obligatoire\")\n",
        "print(\"  • Investigation des causes\")\n",
        "\n",
        "print(\"\\nALERTE ORANGE (erreur 10-20%):\")\n",
        "print(\"  • Validation par le manager\")\n",
        "print(\"  • Ajustement possible\")\n",
        "\n",
        "print(\"\\nVERT (erreur <10%):\")\n",
        "print(\"  • Commande automatique\")\n",
        "print(\"  • Monitoring standard\")\n",
        "\n",
        "# Métriques de succès pour le déploiement\n",
        "print(f\"\\nMétriques de succès pour le suivi en production:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"KPIs à surveiller:\")\n",
        "print(f\"  • RMSE < {best_result['rmse_test']*1.1:,.0f}€ (tolérance +10%)\")\n",
        "print(f\"  • MAE < {best_result['mae_test']*1.1:,.0f}€\")\n",
        "print(f\"  • R² > {best_result['r2_test']*0.9:.3f} (tolérance -10%)\")\n",
        "print(f\"  • Erreur relative moyenne < 15%\")\n",
        "\n",
        "print(f\"\\nFréquence de réentraînement:\")\n",
        "print(\"  • Réentraînement mensuel recommandé\")\n",
        "print(\"  • Réentraînement d'urgence si performance dégradée\")\n",
        "print(\"  • Intégration de nouvelles features trimestrielle\")\n",
        "\n",
        "# ROI du projet\n",
        "print(f\"\\nROI estimé du projet:\")\n",
        "print(\"=\" * 30)\n",
        "economie_annuelle = economie * (52 / (len(X_test) // n_stores))  # Extrapoler sur 1 an\n",
        "cout_developpement = 50000  # Estimation\n",
        "roi = (economie_annuelle - cout_developpement) / cout_developpement * 100\n",
        "\n",
        "print(f\"  Économie annuelle estimée: {economie_annuelle:,.0f}€\")\n",
        "print(f\"  Coût de développement: {cout_developpement:,.0f}€\")\n",
        "print(f\"  ROI première année: {roi:.0f}%\")\n",
        "\n",
        "if roi > 100:\n",
        "    print(\"  ✅ Projet très rentable!\")\n",
        "elif roi > 50:\n",
        "    print(\"  ✅ Projet rentable\")\n",
        "elif roi > 0:\n",
        "    print(\"  ⚠️ Projet marginalement rentable\")\n",
        "else:\n",
        "    print(\"  ❌ Projet non rentable en l'état\")"
      ]
    }
  ]
}