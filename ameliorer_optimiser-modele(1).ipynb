{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ab84b-4de2-407a-b7ae-7a2f09b30473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETER TUNING\n",
      "=========================\n",
      "Dataset : Train (800, 10), Test (200, 10)\n",
      "\n",
      "1. MODÈLE DE BASE\n",
      "Modèle de base (paramètres par défaut) :\n",
      "   Train accuracy : 1.000\n",
      "   Test accuracy  : 0.950\n",
      "   Gap            : 0.050\n",
      "\n",
      "Paramètres par défaut :\n",
      "   n_estimators: 100\n",
      "   max_depth: None\n",
      "   min_samples_split: 2\n",
      "   min_samples_leaf: 1\n",
      "   max_features: sqrt\n",
      "\n",
      "2. GRID SEARCH\n",
      "Grille de paramètres :\n",
      "   n_estimators: [50, 100, 200]\n",
      "   max_depth: [5, 10, 15, None]\n",
      "   min_samples_split: [2, 5, 10]\n",
      "   min_samples_leaf: [1, 2, 4]\n",
      "\n",
      "Nombre total de combinaisons : 108\n",
      "Lancement du Grid Search (cela peut prendre du temps)...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning avec Grid Search et Random Search\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV, \n",
    "    cross_val_score, StratifiedKFold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 25)\n",
    "#----------------------------------------------\n",
    "# Créer un dataset\n",
    "#----------------------------------------------\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset : Train {X_train.shape}, Test {X_test.shape}\")\n",
    "#----------------------------------------------\n",
    "# 1. MODÈLE DE BASE (SANS TUNING)\n",
    "#----------------------------------------------\n",
    "print(f\"\\n1. MODÈLE DE BASE\")\n",
    "#----------------------------------------------\n",
    "# Modèle avec paramètres par défaut\n",
    "#----------------------------------------------\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "base_train_score = base_model.score(X_train, y_train)\n",
    "base_test_score = base_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Modèle de base (paramètres par défaut) :\")\n",
    "print(f\"   Train accuracy : {base_train_score:.3f}\")\n",
    "print(f\"   Test accuracy  : {base_test_score:.3f}\")\n",
    "print(f\"   Gap            : {base_train_score - base_test_score:.3f}\")\n",
    "#----------------------------------------------\n",
    "# Afficher paramètres par défaut\n",
    "#----------------------------------------------\n",
    "print(\"\\nParamètres par défaut :\")\n",
    "default_params = base_model.get_params()\n",
    "key_params = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
    "for param in key_params:\n",
    "    print(f\"   {param}: {default_params.get(param, 'N/A')}\")\n",
    "#----------------------------------------------\n",
    "# 2. GRID SEARCH\n",
    "#----------------------------------------------\n",
    "\n",
    "print(f\"\\n2. GRID SEARCH\")\n",
    "#----------------------------------------------\n",
    "# Définir la grille de paramètres\n",
    "#----------------------------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Grille de paramètres :\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(values) for values in param_grid.values()])\n",
    "print(f\"\\nNombre total de combinaisons : {total_combinations}\")\n",
    "#----------------------------------------------\n",
    "# Grid Search avec validation croisée\n",
    "#----------------------------------------------\n",
    "print(\"Lancement du Grid Search (cela peut prendre du temps)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Utiliser tous les processeurs\n",
    "    verbose=1   # Afficher le progrès\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"Grid Search terminé en {grid_time:.1f} secondes\")\n",
    "\n",
    "# Meilleurs paramètres\n",
    "best_params_grid = grid_search.best_params_\n",
    "best_score_grid = grid_search.best_score_\n",
    "\n",
    "print(\"\\nMeilleurs paramètres (Grid Search) :\")\n",
    "for param, value in best_params_grid.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"   Score CV : {best_score_grid:.3f}\")\n",
    "\n",
    "# Évaluer sur le test set\n",
    "best_model_grid = grid_search.best_estimator_\n",
    "grid_test_score = best_model_grid.score(X_test, y_test)\n",
    "grid_train_score = best_model_grid.score(X_train, y_train)\n",
    "\n",
    "print(\"\\nPerformances sur test set :\")\n",
    "print(f\"   Train accuracy : {grid_train_score:.3f}\")\n",
    "print(f\"   Test accuracy  : {grid_test_score:.3f}\")\n",
    "print(f\"   Gap            : {grid_train_score - grid_test_score:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 3. RANDOM SEARCH (PLUS EFFICACE)\n",
    "#----------------------------------------------\n",
    "\n",
    "print(f\"\\n3. RANDOM SEARCH\")\n",
    "\n",
    "# Définir des distributions pour Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.8]\n",
    "}\n",
    "\n",
    "print(\"Distributions de paramètres :\")\n",
    "for param, values in param_dist.items():\n",
    "    print(f\"   {param}: {len(values)} valeurs\")\n",
    "\n",
    "# Random Search\n",
    "n_iter = 50  # Nombre d'itérations (combinaisons à tester)\n",
    "print(f\"\\nRandom Search avec {n_iter} itérations...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=n_iter,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"Random Search terminé en {random_time:.1f} secondes\")\n",
    "\n",
    "# Meilleurs paramètres\n",
    "best_params_random = random_search.best_params_\n",
    "best_score_random = random_search.best_score_\n",
    "\n",
    "print(\"\\nMeilleurs paramètres (Random Search) :\")\n",
    "for param, value in best_params_random.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"   Score CV : {best_score_random:.3f}\")\n",
    "\n",
    "# Évaluer sur le test set\n",
    "best_model_random = random_search.best_estimator_\n",
    "random_test_score = best_model_random.score(X_test, y_test)\n",
    "random_train_score = best_model_random.score(X_train, y_train)\n",
    "\n",
    "print(\"\\nPerformances sur test set :\")\n",
    "print(f\"   Train accuracy : {random_train_score:.3f}\")\n",
    "print(f\"   Test accuracy  : {random_test_score:.3f}\")\n",
    "print(f\"   Gap            : {random_train_score - random_test_score:.3f}\")\n",
    "#----------------------------------------------\n",
    "# 4. COMPARAISON DES MÉTHODES\n",
    "#----------------------------------------------\n",
    "\n",
    "print(f\"\\n4. COMPARAISON DES MÉTHODES\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Méthode': ['Baseline', 'Grid Search', 'Random Search'],\n",
    "    'Train Accuracy': [base_train_score, grid_train_score, random_train_score],\n",
    "    'Test Accuracy': [base_test_score, grid_test_score, random_test_score],\n",
    "    'Gap': [\n",
    "        base_train_score - base_test_score,\n",
    "        grid_train_score - grid_test_score,\n",
    "        random_train_score - random_test_score\n",
    "    ],\n",
    "    'Temps (s)': [0, grid_time, random_time]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(3)\n",
    "print(comparison_df)\n",
    "\n",
    "# Amélioration par rapport au baseline\n",
    "grid_improvement = grid_test_score - base_test_score\n",
    "random_improvement = random_test_score - base_test_score\n",
    "\n",
    "print(\"\\nAméliorations :\")\n",
    "print(f\"   Grid Search    : +{grid_improvement:.3f} ({grid_improvement*100:.1f}%)\")\n",
    "print(f\"   Random Search  : +{random_improvement:.3f} ({random_improvement*100:.1f}%)\")\n",
    "\n",
    "# 5. VISUALISATION DES RÉSULTATS\n",
    "print(f\"\\n5. VISUALISATION\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Graphique 1 : Comparaison des accuracies\n",
    "methods = comparison_df['Méthode']\n",
    "train_scores = comparison_df['Train Accuracy']\n",
    "test_scores = comparison_df['Test Accuracy']\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_scores, width, label='Train', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Méthode')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Comparaison des Accuracies')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(methods)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2 : Gap train/test\n",
    "gaps = comparison_df['Gap']\n",
    "colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' for gap in gaps]\n",
    "\n",
    "axes[0, 1].bar(methods, gaps, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Gap Train-Test')\n",
    "axes[0, 1].set_title('Overfitting (Gap Train-Test)')\n",
    "axes[0, 1].axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Seuil acceptable')\n",
    "axes[0, 1].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Seuil overfitting')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 3 : Temps d'exécution\n",
    "times = comparison_df['Temps (s)']\n",
    "axes[1, 0].bar(methods, times, alpha=0.8, color='purple')\n",
    "axes[1, 0].set_ylabel('Temps (secondes)')\n",
    "axes[1, 0].set_title('Temps d\\'Exécution')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(times):\n",
    "    if v > 0:\n",
    "        axes[1, 0].text(i, v + max(times)*0.01, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# Graphique 4 : Évolution des scores Grid Search\n",
    "if hasattr(grid_search, 'cv_results_'):\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    scores = cv_results['mean_test_score']\n",
    "    \n",
    "    axes[1, 1].plot(range(len(scores)), sorted(scores, reverse=True), 'o-', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Combinaison (triée par score)')\n",
    "    axes[1, 1].set_ylabel('Score CV')\n",
    "    axes[1, 1].set_title('Distribution des Scores (Grid Search)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------------\n",
    "# 6. ANALYSE DÉTAILLÉE DES PARAMÈTRES\n",
    "#----------------------------------------------\n",
    "\n",
    "print(f\"\\n6. ANALYSE DES PARAMÈTRES\")\n",
    "\n",
    "if hasattr(grid_search, 'cv_results_'):\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    print(\"Impact des paramètres (Grid Search) :\")\n",
    "    for param in ['n_estimators', 'max_depth', 'min_samples_split']:\n",
    "        param_col = f'param_{param}'\n",
    "        if param_col in cv_results.columns:\n",
    "            param_impact = cv_results.groupby(param_col)['mean_test_score'].agg(['mean', 'std'])\n",
    "            print(f\"\\n{param} :\")\n",
    "            for value, stats in param_impact.iterrows():\n",
    "                print(f\"   {value}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 7. RECOMMANDATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n7. RECOMMANDATIONS\")\n",
    "\n",
    "best_method = 'Grid Search' if grid_test_score > random_test_score else 'Random Search'\n",
    "best_score = max(grid_test_score, random_test_score)\n",
    "improvement = best_score - base_test_score\n",
    "\n",
    "print(f\"Meilleure méthode : {best_method}\")\n",
    "print(f\"Amélioration : +{improvement:.3f} ({improvement*100:.1f}%)\")\n",
    "\n",
    "recommendations = f\"\"\"\n",
    "RECOMMANDATIONS HYPERPARAMETER TUNING :\n",
    "\n",
    "EFFICACITÉ :\n",
    "   • Random Search souvent plus efficace que Grid Search\n",
    "   • Commencer par Random Search avec beaucoup d'itérations\n",
    "   • Affiner avec Grid Search sur une zone restreinte\n",
    "\n",
    "PARAMÈTRES CLÉS :\n",
    "   • n_estimators : Plus = mieux (mais + lent)\n",
    "   • max_depth : Contrôle l'overfitting\n",
    "   • min_samples_split/leaf : Régularisation\n",
    "   • max_features : Impact sur diversité\n",
    "\n",
    "VALIDATION :\n",
    "   • Toujours utiliser validation croisée\n",
    "   • Surveiller le gap train/test\n",
    "   • Tester sur un vrai test set à la fin\n",
    "\n",
    "OPTIMISATION :\n",
    "   • Utiliser n_jobs=-1 pour parallélisation\n",
    "   • Commencer par paramètres ayant + d'impact\n",
    "   • Itérer : grossier puis fin\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "print(\"\\nHyperparameter tuning terminé !\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# Feature Engineering avancé\n",
    "#----------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"FEATURE ENGINEERING AVANCÉ\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Créer un dataset avec plus de features\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset initial : {X_train.shape[1]} features\")\n",
    "\n",
    "# Modèle de référence\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_score = cross_val_score(base_model, X_train, y_train, cv=5).mean()\n",
    "print(f\"Score de référence : {base_score:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 1. FEATURES POLYNOMIALES\n",
    "#----------------------------------------------\n",
    "print(f\"\\n1. FEATURES POLYNOMIALES\")\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_train[:, :5])  # Seulement sur 5 premières features\n",
    "\n",
    "print(\"Features polynomiales :\")\n",
    "print(\"   Original : 5 features\")\n",
    "print(f\"   Polynomial : {X_poly.shape[1]} features\")\n",
    "\n",
    "# Tester l'amélioration\n",
    "X_train_extended = np.hstack([X_train, X_poly])\n",
    "X_test_poly = poly.transform(X_test[:, :5])\n",
    "X_test_extended = np.hstack([X_test, X_test_poly])\n",
    "\n",
    "poly_score = cross_val_score(base_model, X_train_extended, y_train, cv=5).mean()\n",
    "improvement_poly = poly_score - base_score\n",
    "\n",
    "print(f\"Score avec features polynomiales : {poly_score:.3f}\")\n",
    "print(f\"Amélioration : {improvement_poly:+.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 2. SÉLECTION DE FEATURES\n",
    "#----------------------------------------------\n",
    "print(f\"\\n2. SÉLECTION DE FEATURES\")\n",
    "\n",
    "selector_kbest = SelectKBest(k=10)\n",
    "X_train_kbest = selector_kbest.fit_transform(X_train, y_train)\n",
    "X_test_kbest = selector_kbest.transform(X_test)\n",
    "\n",
    "kbest_score = cross_val_score(base_model, X_train_kbest, y_train, cv=5).mean()\n",
    "improvement_kbest = kbest_score - base_score\n",
    "\n",
    "print(\"SelectKBest (10 meilleures features) :\")\n",
    "print(f\"   Score : {kbest_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_kbest:+.3f}\")\n",
    "selected_features = selector_kbest.get_support()\n",
    "print(f\"   Features sélectionnées : {np.where(selected_features)[0]}\")\n",
    "\n",
    "# Méthode 2 : RFE\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=50, random_state=42), n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "rfe_score = cross_val_score(base_model, X_train_rfe, y_train, cv=5).mean()\n",
    "improvement_rfe = rfe_score - base_score\n",
    "\n",
    "print(\"\\nRFE (10 meilleures features) :\")\n",
    "print(f\"   Score : {rfe_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_rfe:+.3f}\")\n",
    "print(f\"   Features sélectionnées : {np.where(rfe.support_)[0]}\")\n",
    "\n",
    "# Méthode 3 : SelectFromModel (Lasso)\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "selector_model = SelectFromModel(lasso)\n",
    "X_train_model = selector_model.fit_transform(X_train, y_train)\n",
    "X_test_model = selector_model.transform(X_test)\n",
    "\n",
    "model_score = cross_val_score(base_model, X_train_model, y_train, cv=5).mean()\n",
    "improvement_model = model_score - base_score\n",
    "\n",
    "print(\"\\nSelectFromModel (Lasso) :\")\n",
    "print(f\"   Features sélectionnées : {X_train_model.shape[1]}\")\n",
    "print(f\"   Score : {model_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_model:+.3f}\")\n",
    "\n",
    "# 3. RÉDUCTION DE DIMENSIONALITÉ\n",
    "print(f\"\\n3. RÉDUCTION DE DIMENSIONALITÉ\")\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_score = cross_val_score(base_model, X_train_pca, y_train, cv=5).mean()\n",
    "improvement_pca = pca_score - base_score\n",
    "\n",
    "print(\"PCA (10 composantes) :\")\n",
    "print(f\"   Score : {pca_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_pca:+.3f}\")\n",
    "print(f\"   Variance expliquée : {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Visualisations\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(range(1, 11), pca.explained_variance_ratio_, 'o-')\n",
    "plt.xlabel('Composante'); plt.ylabel('Variance Expliquée'); plt.title('Variance Expliquée par Composante PCA'); plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_), 'o-', color='red')\n",
    "plt.xlabel('Nombre de Composantes'); plt.ylabel('Variance Cumulée'); plt.title('Variance Cumulée PCA'); plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. COMBINAISON\n",
    "print(f\"\\n4. COMBINAISON DE TECHNIQUES\")\n",
    "\n",
    "X_train_combined = X_train_kbest\n",
    "pca_combined = PCA(n_components=8)\n",
    "X_train_combined = pca_combined.fit_transform(X_train_combined)\n",
    "X_test_combined = pca_combined.transform(X_test_kbest)\n",
    "\n",
    "combined_score = cross_val_score(base_model, X_train_combined, y_train, cv=5).mean()\n",
    "improvement_combined = combined_score - base_score\n",
    "\n",
    "print(\"Combiné (SelectKBest + PCA) :\")\n",
    "print(f\"   Features finales : {X_train_combined.shape[1]}\")\n",
    "print(f\"   Score : {combined_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_combined:+.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 5. FEATURES PERSONNALISÉES\n",
    "#----------------------------------------------\n",
    "print(f\"\\n5. FEATURES PERSONNALISÉES\")\n",
    "\n",
    "def create_custom_features(X):\n",
    "    X_custom = X.copy()\n",
    "    X_custom = np.column_stack([X_custom, X[:, 0] / (X[:, 1] + 1e-8)])\n",
    "    X_custom = np.column_stack([X_custom, X[:, 2] / (X[:, 3] + 1e-8)])\n",
    "    X_custom = np.column_stack([X_custom, X[:, 0] + X[:, 1]])\n",
    "    X_custom = np.column_stack([X_custom, np.abs(X[:, 0] - X[:, 1])])\n",
    "    X_custom = np.column_stack([X_custom, np.mean(X, axis=1)])\n",
    "    X_custom = np.column_stack([X_custom, np.std(X, axis=1)])\n",
    "    X_custom = np.column_stack([X_custom, np.max(X, axis=1)])\n",
    "    X_custom = np.column_stack([X_custom, np.min(X, axis=1)])\n",
    "    return X_custom\n",
    "\n",
    "X_train_custom = create_custom_features(X_train)\n",
    "X_test_custom = create_custom_features(X_test)\n",
    "\n",
    "custom_score = cross_val_score(base_model, X_train_custom, y_train, cv=5).mean()\n",
    "improvement_custom = custom_score - base_score\n",
    "\n",
    "print(\"Features personnalisées :\")\n",
    "print(f\"   Features ajoutées : {X_train_custom.shape[1] - X_train.shape[1]}\")\n",
    "print(f\"   Score : {custom_score:.3f}\")\n",
    "print(f\"   Amélioration : {improvement_custom:+.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 6. COMPARAISON\n",
    "#----------------------------------------------\n",
    "print(f\"\\n6. COMPARAISON DE TOUTES LES MÉTHODES\")\n",
    "\n",
    "methods_data = {\n",
    "    'Méthode': ['Baseline','Polynomial Features','SelectKBest','RFE','SelectFromModel','PCA','Combiné','Features Personnalisées'],\n",
    "    'Score': [base_score, poly_score, kbest_score, rfe_score, model_score, pca_score, combined_score, custom_score],\n",
    "    'Amélioration': [0, improvement_poly, improvement_kbest, improvement_rfe, improvement_model, improvement_pca, improvement_combined, improvement_custom],\n",
    "    'Nb Features': [X_train.shape[1], X_train_extended.shape[1], X_train_kbest.shape[1], X_train_rfe.shape[1], X_train_model.shape[1], X_train_pca.shape[1], X_train_combined.shape[1], X_train_custom.shape[1]]\n",
    "}\n",
    "methods_df = pd.DataFrame(methods_data).round(3)\n",
    "print(methods_df)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "methods = methods_df['Méthode']; scores = methods_df['Score']; improvements = methods_df['Amélioration']\n",
    "colors = ['red' if imp < 0 else 'green' if imp > 0.01 else 'orange' for imp in improvements]\n",
    "bars = plt.bar(range(len(methods)), scores, color=colors, alpha=0.8)\n",
    "plt.xlabel('Méthode'); plt.ylabel('Score'); plt.title('Comparaison des Méthodes')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right'); plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=base_score, color='blue', linestyle='--', alpha=0.7, label='Baseline'); plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.bar(range(len(methods)), improvements, color=colors, alpha=0.8)\n",
    "plt.xlabel('Méthode'); plt.ylabel('Amélioration'); plt.title('Amélioration vs Baseline')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right'); plt.axhline(y=0, color='black', linestyle='-', alpha=0.5); plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "nb_features = methods_df['Nb Features']\n",
    "plt.bar(range(len(methods)), nb_features, alpha=0.8, color='purple')\n",
    "plt.xlabel('Méthode'); plt.ylabel('Nombre de Features'); plt.title('Nombre de Features par Méthode')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right'); plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(nb_features, scores, s=100, alpha=0.7, color=colors)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.annotate(method, (nb_features[i], scores[i]), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.xlabel('Nombre de Features'); plt.ylabel('Score'); plt.title('Score vs Complexité'); plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "#----------------------------------------------\n",
    "# 7. RECOMMANDATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n7. RECOMMANDATIONS\")\n",
    "best_method_idx = methods_df['Score'].idxmax()\n",
    "best_method = methods_df.loc[best_method_idx]\n",
    "print(f\"Meilleure méthode : {best_method['Méthode']}\")\n",
    "print(f\"Score : {best_method['Score']:.3f}\")\n",
    "print(f\"Amélioration : {best_method['Amélioration']:+.3f}\")\n",
    "print(f\"Features : {best_method['Nb Features']}\")\n",
    "\n",
    "feature_recommendations = f\"\"\"\n",
    "RECOMMANDATIONS FEATURE ENGINEERING :\n",
    "\n",
    "SÉLECTION :\n",
    "   • SelectKBest : Rapide, bon point de départ\n",
    "   • RFE : Plus précis mais plus lent\n",
    "   • SelectFromModel : Importance du modèle\n",
    "\n",
    "RÉDUCTION :\n",
    "   • PCA : Perd l'interprétabilité mais efficace\n",
    "   • Combiné : Sélection puis réduction souvent optimal\n",
    "\n",
    "CRÉATION :\n",
    "   • Features métier\n",
    "   • Ratios et interactions\n",
    "   • Statistiques par échantillon : mean, std, min, max\n",
    "\n",
    "ÉQUILIBRE :\n",
    "   • Plus de features ≠ toujours mieux\n",
    "   • Surveiller l'overfitting\n",
    "   • Valider sur test set indépendant\n",
    "\"\"\"\n",
    "print(feature_recommendations)\n",
    "print(\"\\nFeature Engineering avancé terminé !\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# Ensemble Methods - Combiner plusieurs modèles\n",
    "#----------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    VotingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ENSEMBLE METHODS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset : Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 1. MODÈLES INDIVIDUELS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n1. MODÈLES INDIVIDUELS\")\n",
    "\n",
    "individual_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "individual_results = {}\n",
    "for name, model in individual_models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean(); cv_std = cv_scores.std()\n",
    "    model.fit(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    individual_results[name] = {'cv_mean': cv_mean, 'cv_std': cv_std, 'test_score': test_score, 'model': model}\n",
    "    print(f\"   {name:<20} : CV {cv_mean:.3f} ± {cv_std:.3f}, Test {test_score:.3f}\")\n",
    "#----------------------------------------------\n",
    "# 2. VOTING CLASSIFIER (HARD)\n",
    "#----------------------------------------------\n",
    "print(f\"\\n2. VOTING CLASSIFIER (HARD)\")\n",
    "\n",
    "best_models = sorted(individual_results.items(), key=lambda x: x[1]['cv_mean'], reverse=True)[:3]\n",
    "print(\"Meilleurs modèles sélectionnés :\")\n",
    "for name, results in best_models:\n",
    "    print(f\"   {name} : {results['cv_mean']:.3f}\")\n",
    "\n",
    "voting_models = [(name, results['model']) for name, results in best_models]\n",
    "hard_voting = VotingClassifier(estimators=voting_models, voting='hard')\n",
    "hard_cv_scores = cross_val_score(hard_voting, X_train, y_train, cv=5)\n",
    "hard_voting.fit(X_train, y_train)\n",
    "hard_test_score = hard_voting.score(X_test, y_test)\n",
    "print(\"\\nHard Voting :\")\n",
    "print(f\"   CV : {hard_cv_scores.mean():.3f} ± {hard_cv_scores.std():.3f}\")\n",
    "print(f\"   Test : {hard_test_score:.3f}\")\n",
    "#----------------------------------------------\n",
    "# 3. VOTING CLASSIFIER (SOFT)\n",
    "#----------------------------------------------\n",
    "print(f\"\\n3. VOTING CLASSIFIER (SOFT)\")\n",
    "soft_voting = VotingClassifier(estimators=voting_models, voting='soft')\n",
    "soft_cv_scores = cross_val_score(soft_voting, X_train, y_train, cv=5)\n",
    "soft_voting.fit(X_train, y_train)\n",
    "soft_test_score = soft_voting.score(X_test, y_test)\n",
    "print(\"Soft Voting :\")\n",
    "print(f\"   CV : {soft_cv_scores.mean():.3f} ± {soft_cv_scores.std():.3f}\")\n",
    "print(f\"   Test : {soft_test_score:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 4. BAGGING\n",
    "#----------------------------------------------\n",
    "print(f\"\\n4. BAGGING\")\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, random_state=42)\n",
    "bagging_cv_scores = cross_val_score(bagging, X_train, y_train, cv=5)\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_test_score = bagging.score(X_test, y_test)\n",
    "print(\"Bagging (100 Decision Trees) :\")\n",
    "print(f\"   CV : {bagging_cv_scores.mean():.3f} ± {bagging_cv_scores.std():.3f}\")\n",
    "print(f\"   Test : {bagging_test_score:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 5. STACKING SIMPLE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n5. STACKING SIMPLE\")\n",
    "def simple_stacking(models, X_train, y_train, X_test, y_test):\n",
    "    base_predictions_train = []\n",
    "    base_predictions_test = []\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            train_pred = model.predict(X_train)\n",
    "            test_pred = model.predict(X_test)\n",
    "        base_predictions_train.append(train_pred); base_predictions_test.append(test_pred)\n",
    "    meta_X_train = np.column_stack(base_predictions_train)\n",
    "    meta_X_test = np.column_stack(base_predictions_test)\n",
    "    meta_model = LogisticRegression(random_state=42)\n",
    "    meta_model.fit(meta_X_train, y_train)\n",
    "    final_predictions = meta_model.predict(meta_X_test)\n",
    "    stacking_score = accuracy_score(y_test, final_predictions)\n",
    "    return stacking_score, meta_model\n",
    "\n",
    "stacking_score, meta_model = simple_stacking(voting_models, X_train, y_train, X_test, y_test)\n",
    "print(\"Stacking Simple :\")\n",
    "print(f\"   Test : {stacking_score:.3f}\")\n",
    "#----------------------------------------------\n",
    "# 6. COMPARAISON COMPLÈTE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n6. COMPARAISON COMPLÈTE\")\n",
    "all_results = {name: results['test_score'] for name, results in individual_results.items()}\n",
    "all_results['Hard Voting'] = hard_test_score\n",
    "all_results['Soft Voting'] = soft_test_score\n",
    "all_results['Bagging'] = bagging_test_score\n",
    "all_results['Stacking'] = stacking_score\n",
    "\n",
    "results_df = pd.DataFrame(list(all_results.items()), columns=['Méthode', 'Test Score']).sort_values('Test Score', ascending=False)\n",
    "print(\"Classement final :\")\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    medal = f\"{i}.\"\n",
    "    print(f\"   {medal} {row['Méthode']:<20} : {row['Test Score']:.3f}\")\n",
    "#----------------------------------------------\n",
    "# 7. VISUALISATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n7. VISUALISATIONS\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "methods = results_df['Méthode']; scores = results_df['Test Score']\n",
    "colors = ['gold' if m in ['Hard Voting','Soft Voting','Bagging','Stacking'] else 'lightblue' for m in methods]\n",
    "axes[0, 0].barh(methods, scores, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Test Score'); axes[0, 0].set_title('Comparaison de Toutes les Méthodes'); axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, score in enumerate(scores): axes[0, 0].text(score + 0.001, i, f'{score:.3f}', va='center')\n",
    "\n",
    "baseline_score = max([r['test_score'] for r in individual_results.values()])\n",
    "ensemble_methods = ['Hard Voting','Soft Voting','Bagging','Stacking']\n",
    "ensemble_scores = [all_results[m] for m in ensemble_methods]\n",
    "improvements = [s - baseline_score for s in ensemble_scores]\n",
    "colors_improvement = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "axes[0, 1].bar(ensemble_methods, improvements, color=colors_improvement, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Amélioration vs Meilleur Individuel'); axes[0, 1].set_title('Gain des Ensemble Methods')\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5); axes[0, 1].grid(True, alpha=0.3); axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "cv_means = [individual_results[n]['cv_mean'] for n in individual_results.keys()]\n",
    "cv_stds  = [individual_results[n]['cv_std']  for n in individual_results.keys()]\n",
    "model_names = list(individual_results.keys())\n",
    "axes[1, 0].errorbar(range(len(model_names)), cv_means, yerr=cv_stds, fmt='o', capsize=5, capthick=2)\n",
    "axes[1, 0].set_xticks(range(len(model_names))); axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('CV Score'); axes[1, 0].set_title('Variance des Modèles Individuels'); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "model_predictions = {name: results['model'].predict(X_test) for name, results in individual_results.items()}\n",
    "pred_df = pd.DataFrame(model_predictions); correlation_matrix = pred_df.corr()\n",
    "im = axes[1, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "axes[1, 1].set_xticks(range(len(model_names))); axes[1, 1].set_yticks(range(len(model_names)))\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right'); axes[1, 1].set_yticklabels(model_names)\n",
    "axes[1, 1].set_title('Corrélation des Prédictions')\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(model_names)):\n",
    "        axes[1, 1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha='center', va='center')\n",
    "plt.colorbar(im, ax=axes[1, 1]); plt.tight_layout(); plt.show()\n",
    "\n",
    "#----------------------------------------------\n",
    "# 8. RECOMMANDATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n8. RECOMMANDATIONS\")\n",
    "best_ensemble = max(ensemble_methods, key=lambda x: all_results[x])\n",
    "best_improvement = max(improvements)\n",
    "\n",
    "recommendations = f\"\"\"\n",
    "RECOMMANDATIONS ENSEMBLE METHODS :\n",
    "\n",
    "RÉSULTATS :\n",
    "   • Meilleur ensemble : {best_ensemble}\n",
    "   • Amélioration max : {best_improvement:+.3f}\n",
    "   • Toujours tester plusieurs approches\n",
    "\n",
    "QUAND UTILISER :\n",
    "   • Soft Voting : Modèles avec probabilités\n",
    "   • Hard Voting : Modèles très différents\n",
    "   • Bagging : Réduire variance (Decision Trees)\n",
    "   • Stacking : Maximum de performance\n",
    "\n",
    "DIVERSITÉ :\n",
    "   • Combiner modèles différents\n",
    "   • Vérifier corrélation des prédictions\n",
    "   • Plus de diversité = meilleur ensemble\n",
    "\n",
    "BONNES PRATIQUES :\n",
    "   • Validation croisée pour sélection\n",
    "   • Pas plus de 3-5 modèles en général\n",
    "   • Équilibrer performance et complexité\n",
    "   • Tester sur vrai test set à la fin\n",
    "\"\"\"\n",
    "print(recommendations)\n",
    "print(\"\\nEnsemble Methods terminé !\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# Pipeline d'optimisation complet\n",
    "#----------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"PIPELINE D'OPTIMISATION COMPLET\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Dataset plus complexe\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=25,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset : {X_train.shape[1]} features, {len(X_train)} échantillons train\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 1. PIPELINE DE BASE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n1. PIPELINE DE BASE\")\n",
    "\n",
    "base_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectKBest(k=15)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "base_pipeline.fit(X_train, y_train)\n",
    "base_score = base_pipeline.score(X_test, y_test)\n",
    "base_time = time.time() - start_time\n",
    "\n",
    "print(\"Pipeline de base :\")\n",
    "print(f\"   Score : {base_score:.3f}\")\n",
    "print(f\"   Temps : {base_time:.2f}s\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 2. OPTIMISATION DES HYPERPARAMÈTRES\n",
    "#----------------------------------------------\n",
    "print(f\"\\n2. OPTIMISATION HYPERPARAMÈTRES\")\n",
    "\n",
    "param_grid = {\n",
    "    'selector__k': [10, 15, 20],\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 15, None],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(\"Grille de paramètres :\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(values) for values in param_grid.values()])\n",
    "print(f\"Total combinaisons : {total_combinations}\")\n",
    "\n",
    "print(\"Lancement Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    base_pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "best_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Grid Search terminé en {grid_time:.1f}s\")\n",
    "print(\"\\nMeilleurs paramètres :\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"Score CV : {best_cv_score:.3f}\")\n",
    "print(f\"Score Test : {best_test_score:.3f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 3. PIPELINE ENSEMBLE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n3. PIPELINE ENSEMBLE\")\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectKBest(k=best_params['selector__k'])),\n",
    "    ('classifier', VotingClassifier([\n",
    "        ('rf', RandomForestClassifier(\n",
    "            n_estimators=best_params['classifier__n_estimators'],\n",
    "            max_depth=best_params['classifier__max_depth'],\n",
    "            min_samples_split=best_params['classifier__min_samples_split'],\n",
    "            random_state=42\n",
    "        )),\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ], voting='soft'))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "ensemble_score = ensemble_pipeline.score(X_test, y_test)\n",
    "ensemble_time = time.time() - start_time\n",
    "\n",
    "print(\"Pipeline Ensemble :\")\n",
    "print(f\"   Score : {ensemble_score:.3f}\")\n",
    "print(f\"   Temps : {ensemble_time:.2f}s\")\n",
    "\n",
    "#----------------------------------------------\n",
    "# 4. ANALYSE COMPLÈTE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n4. ANALYSE COMPLÈTE\")\n",
    "\n",
    "y_pred_base = base_pipeline.predict(X_test)\n",
    "y_pred_optimized = grid_search.predict(X_test)\n",
    "y_pred_ensemble = ensemble_pipeline.predict(X_test)\n",
    "\n",
    "def detailed_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "base_metrics = detailed_metrics(y_test, y_pred_base, \"Base\")\n",
    "optimized_metrics = detailed_metrics(y_test, y_pred_optimized, \"Optimized\")\n",
    "ensemble_metrics = detailed_metrics(y_test, y_pred_ensemble, \"Ensemble\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Pipeline': ['Base', 'Optimisé', 'Ensemble'],\n",
    "    'Accuracy': [base_metrics['accuracy'], optimized_metrics['accuracy'], ensemble_metrics['accuracy']],\n",
    "    'Precision': [base_metrics['precision'], optimized_metrics['precision'], ensemble_metrics['precision']],\n",
    "    'Recall': [base_metrics['recall'], optimized_metrics['recall'], ensemble_metrics['recall']],\n",
    "    'F1-Score': [base_metrics['f1'], optimized_metrics['f1'], ensemble_metrics['f1']],\n",
    "    'Temps (s)': [base_time, grid_time, ensemble_time]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data).round(3)\n",
    "print(\"\\nComparaison finale :\")\n",
    "print(comparison_df)\n",
    "\n",
    "#----------------------------------------------\n",
    "# 5. ANALYSE DES AMÉLIORATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n5. ANALYSE DES AMÉLIORATIONS\")\n",
    "base_acc = base_metrics['accuracy']\n",
    "opt_improvement = optimized_metrics['accuracy'] - base_acc\n",
    "ens_improvement = ensemble_metrics['accuracy'] - base_acc\n",
    "print(\"Améliorations par rapport au baseline :\")\n",
    "print(f\"   Optimisation : {opt_improvement:+.3f} ({opt_improvement*100:+.1f}%)\")\n",
    "print(f\"   Ensemble : {ens_improvement:+.3f} ({ens_improvement*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\nAnalyse temps vs performance :\")\n",
    "print(f\"   Base : {base_acc:.3f} en {base_time:.1f}s\")\n",
    "print(f\"   Optimisé : {optimized_metrics['accuracy']:.3f} en {grid_time:.1f}s ({grid_time/base_time:.1f}x plus lent)\")\n",
    "print(f\"   Ensemble : {ensemble_metrics['accuracy']:.3f} en {ensemble_time:.1f}s\")\n",
    "#----------------------------------------------\n",
    "# 6. VISUALISATIONS\n",
    "#----------------------------------------------\n",
    "print(f\"\\n6. VISUALISATIONS\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy','Precision','Recall','F1-Score']\n",
    "base_values = [base_metrics[m.lower()] for m in metrics]\n",
    "opt_values = [optimized_metrics[m.lower()] for m in metrics]\n",
    "ens_values = [ensemble_metrics[m.lower()] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics)); width = 0.25\n",
    "axes[0, 0].bar(x - width, base_values, width, label='Base', alpha=0.8)\n",
    "axes[0, 0].bar(x, opt_values, width, label='Optimisé', alpha=0.8)\n",
    "axes[0, 0].bar(x + width, ens_values, width, label='Ensemble', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Métriques'); axes[0, 0].set_ylabel('Score'); axes[0, 0].set_title('Comparaison des Métriques')\n",
    "axes[0, 0].set_xticks(x); axes[0, 0].set_xticklabels(metrics); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "pipelines = ['Base','Optimisé','Ensemble']; times = [base_time, grid_time, ensemble_time]; colors = ['green','orange','red']\n",
    "bars = axes[0, 1].bar(pipelines, times, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Temps (secondes)'); axes[0, 1].set_title('Temps d\\'Exécution'); axes[0, 1].grid(True, alpha=0.3)\n",
    "for bar, time_val in zip(bars, times):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01, f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "improvements = [0, opt_improvement, ens_improvement]; colors_imp = ['gray','blue','green']\n",
    "axes[1, 0].bar(pipelines, improvements, color=colors_imp, alpha=0.8)\n",
    "axes[1, 0].set_ylabel('Amélioration Accuracy'); axes[1, 0].set_title('Amélioration vs Baseline')\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "accuracies = [base_acc, optimized_metrics['accuracy'], ensemble_metrics['accuracy']]\n",
    "axes[1, 1].scatter(times, accuracies, s=200, c=colors, alpha=0.8)\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    axes[1, 1].annotate(pipeline, (times[i], accuracies[i]), xytext=(5, 5), textcoords='offset points')\n",
    "axes[1, 1].set_xlabel('Temps (secondes)'); axes[1, 1].set_ylabel('Accuracy'); axes[1, 1].set_title('Performance vs Temps')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "#----------------------------------------------\n",
    "# 7. RECOMMANDATIONS FINALES\n",
    "#----------------------------------------------\n",
    "print(f\"\\n7. RECOMMANDATIONS FINALES\")\n",
    "best_pipeline = max([(base_acc, 'Base'), (optimized_metrics['accuracy'], 'Optimisé'), (ensemble_metrics['accuracy'], 'Ensemble')], key=lambda x: x[0])\n",
    "final_recommendations = f\"\"\"\n",
    "STRATÉGIE D'OPTIMISATION COMPLÈTE :\n",
    "\n",
    "RÉSULTATS :\n",
    "   • Meilleur pipeline : {best_pipeline[1]} ({best_pipeline[0]:.3f})\n",
    "   • Amélioration totale : {max(opt_improvement, ens_improvement):+.3f}\n",
    "   • Temps acceptable : {ensemble_time:.1f}s\n",
    "\n",
    "PROCESSUS RECOMMANDÉ :\n",
    "   1. Baseline simple et rapide\n",
    "   2. Hyperparameter tuning ciblé\n",
    "   3. Feature engineering si nécessaire\n",
    "   4. Ensemble methods pour gain final\n",
    "   5. Validation sur test set indépendant\n",
    "\n",
    "ÉQUILIBRES :\n",
    "   • Performance vs Temps\n",
    "   • Complexité vs Interprétabilité\n",
    "   • Gain vs Effort de développement\n",
    "\n",
    "OPTIMISATION CONTINUE :\n",
    "   • Monitorer performance en production\n",
    "   • Réentraîner périodiquement\n",
    "   • Collecter plus de données\n",
    "   • Itérer sur feature engineering\n",
    "\"\"\"\n",
    "print(final_recommendations)\n",
    "\n",
    "#----------------------------------------------\n",
    "# 8. SAUVEGARDE\n",
    "#----------------------------------------------\n",
    "print(f\"\\n8. SAUVEGARDE\")\n",
    "import joblib\n",
    "import inspect\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {\"n_estimators\": 100, \"random_state\": 42}\n",
    "if \"estimator\" in inspect.signature(BaggingClassifier).parameters:\n",
    "    params[\"estimator\"] = DecisionTreeClassifier(random_state=42)\n",
    "else:\n",
    "    params[\"base_estimator\"] = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "bagging = BaggingClassifier(**params)\n",
    "best_model = ensemble_pipeline if ensemble_score > best_test_score else grid_search.best_estimator_\n",
    "model_name = \"ensemble_pipeline.joblib\" if ensemble_score > best_test_score else \"optimized_pipeline.joblib\"\n",
    "joblib.dump(best_model, model_name)\n",
    "print(f\"Meilleur modèle sauvegardé : {model_name}\")\n",
    "print(f\"Score final : {max(ensemble_score, best_test_score):.3f}\")\n",
    "print(\"\\nPipeline d'optimisation complet terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449cae8-efc9-4549-8bdc-5639e053170e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-course)",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
